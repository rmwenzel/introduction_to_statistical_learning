
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{ch09\_notes}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    Table of Contents{}

{{9~~}Support Vector Machines}

{{9.1~~}Maximal Margin Classifier}

{{9.1.1~~}What Is a Hyperplane?}

{{9.1.2~~}Classification Using a Separating Hyperplane}

{{9.1.3~~}The Maximal Margin Classifier}

{{9.1.4~~}Construction of the Maximal Margin Classifier}

{{9.1.5~~}The Non-separable Case}

{{9.2~~}Support Vector Classifiers}

{{9.2.1~~}Overview of the Support Vector Classifier}

{{9.2.2~~}Details of the Support Vector Classifier}

{{9.3~~}Support Vector Machines}

{{9.3.1~~}Classification with Non-Linear Decision Boundaries}

{{9.3.2~~}The Support Vector Machine}

{{9.4~~}SVMs with More than Two Classes}

{{9.4.1~~}One-Versus-One Classification}

{{9.4.2~~}One-Versus-All Classification}

{{9.5~~}Relationship to Logistic Regression}

{{9.6~~}Footnotes}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{support-vector-machines}{%
\section{Support Vector Machines}\label{support-vector-machines}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{maximal-margin-classifier}{%
\subsection{Maximal Margin Classifier}\label{maximal-margin-classifier}}

    \hypertarget{what-is-a-hyperplane}{%
\subsubsection{What Is a Hyperplane?}\label{what-is-a-hyperplane}}

    \begin{itemize}
\tightlist
\item
  A \textbf{\emph{hyperplane}} in \(\mathbb{R}^p\) is an affine subspace
  of dimension \(p-1\). Every hyperplane is the set of solutions \(X\)
  to \(\beta^\top X = 0\) for some \(\beta\in\mathbb{R}^p\).
\item
  A hyperplane \(\beta^\top X = 0\) partitions \(\mathbb{R}^p\) into two
  halfspaces:
\end{itemize}

\[H_+ = \{X\in\mathbb{R}^p\ |\ \beta^\top X > 0\}\]
\[H_- = \{X\in\mathbb{R}^p\ |\ \beta^\top X > 0\}\]

corresponding to either side of the plane, or equivalently,

\[H_+ = \{X\in\mathbb{R}^p\ |\ \text{sgn}(\beta^\top X) =  1\}\]
\[H_- = \{X\in\mathbb{R}^p\ |\ \text{sgn}(\beta^\top X) =  -1\}\]

    \hypertarget{classification-using-a-separating-hyperplane}{%
\subsubsection{Classification Using a Separating
Hyperplane}\label{classification-using-a-separating-hyperplane}}

    \begin{itemize}
\tightlist
\item
  Given data \((x_i, y_i)\), \(i = 1,\dots n\) with response classes
  \(y_i \in \{ \pm 1\}\), a hyperplane \(\beta^\top X = 0\) is
  \textbf{\emph{separating}} if
\end{itemize}

\[\text{sgn}(\beta^\top x_i) = y_i\]

for all \(i\). - Given a separating hyperplane, we may predict

\[\hat{y}_i = \text{sgn}(\beta^\top x_i)\]

    \hypertarget{the-maximal-margin-classifier}{%
\subsubsection{The Maximal Margin
Classifier}\label{the-maximal-margin-classifier}}

    \begin{itemize}
\item
  Separating hyperplanes are not unique (if one exists then uncountably
  many exist). A natural choice is the \textbf{\emph{maximal margin
  hyperplane}} (or \textbf{\emph{optimal separating hyperplane}})
\item
  The \textbf{\emph{margin}} is the minimal perpendicular distance to
  the hyperplane over the sample points
  \[ M = \underset{i}{\min}\{\ ||x_i - P x_i||\ \}\] where \(P\) is the
  projection matrix onto the hyperplane.
\item
  The points \((x_i, y_i)\) ``on the margin'' (where
  \(||x_i - P x_i|| = M\)) are called \textbf{\emph{support vectors}}
\end{itemize}

    \hypertarget{construction-of-the-maximal-margin-classifier}{%
\subsubsection{Construction of the Maximal Margin
Classifier}\label{construction-of-the-maximal-margin-classifier}}

    The maximal margin classifier is the solution to the optimization
problem:

\begin{align*}
\underset{\boldsymbol{\beta}}{\text{argmax}}&\ M\\
\text{subject to}&\ ||\,\boldsymbol{\beta}\,|| = 1\\
& \mathbf{y}^\top(X\boldsymbol{\beta}) \geqslant \mathbf{M}\\
\end{align*}

where \(\mathbf{M} = (M, \dots, M) \in \mathbb{R}^n\) 78 

    \hypertarget{the-non-separable-case}{%
\subsubsection{The Non-separable Case}\label{the-non-separable-case}}

    \begin{itemize}
\item
  The maximal margin classifier is a natural classifier, but a
  separating hyperplane is not guaranteed to exist
\item
  If a separating hyperplane doesn't exist, we can choose an ``almost''
  separating hyperplane by using a ``soft'' margin.
\end{itemize}

    \hypertarget{support-vector-classifiers}{%
\subsection{Support Vector
Classifiers}\label{support-vector-classifiers}}

    \hypertarget{overview-of-the-support-vector-classifier}{%
\subsubsection{Overview of the Support Vector
Classifier}\label{overview-of-the-support-vector-classifier}}

    \begin{itemize}
\item
  Separating hyperplanes don't always exist, and even if they do, they
  may be undesirable.
\item
  The distance to the hyperplane can be thought of as a measure of
  confidence in the classification. For very small margins, the
  separating hyperplane is very sensitive to individual observations --
  we have low confidence in the classification of nearby observations.
\item
  In these situations, we may prefer a hyperplane that doesn't perfectly
  separate in the interest of:

  \begin{itemize}
  \tightlist
  \item
    Greater robustness to individual observations
  \item
    Better classification of most of the training observations
  \end{itemize}
\item
  This is achieved by the \textbf{\emph{support vector classifier}} or
  \textbf{\emph{soft margin classifier}} 79 
\end{itemize}

    \hypertarget{details-of-the-support-vector-classifier}{%
\subsubsection{Details of the Support Vector
Classifier}\label{details-of-the-support-vector-classifier}}

    \begin{itemize}
\tightlist
\item
  The support vector classifier is the solution to the optimization
  problem:
\end{itemize}

\begin{align*}
\underset{\boldsymbol{\beta}}{\text{argmax}}&\ M\\
\text{subject to}&\ ||\,\boldsymbol{\beta}\,|| = 1\\
& y_i(\boldsymbol{\beta}^\top x_i) \geqslant M(1-\epsilon_i)\\
& \epsilon_i \geqslant 0\\
& \sum_i \epsilon_i \leqslant C
\end{align*}

where \(C \geqslant 0\) is a tuning parameter, \(M\) is the margin, and
the \(\epsilon_i\) are slack variables. 80

\begin{itemize}
\tightlist
\item
  Observations on the margin or on the wrong side of the margin are
  called \textbf{\emph{support vectors}}
\end{itemize}

    \hypertarget{support-vector-machines}{%
\subsection{Support Vector Machines}\label{support-vector-machines}}

    \hypertarget{classification-with-non-linear-decision-boundaries}{%
\subsubsection{Classification with Non-Linear Decision
Boundaries}\label{classification-with-non-linear-decision-boundaries}}

    \begin{itemize}
\item
  The support vector classifier is a natural choice for two response
  classes when the class boundary is linear, but may perform poorly when
  the boundary is non-linear.
\item
  Non-linear transformations of the features will lead to a non-linear
  class boundary, but enlarging the feature space too much can lead to
  intractable computations.
\item
  The support vector machine enlarges the feature space in a way which
  is computationally efficient.
\end{itemize}

    \hypertarget{the-support-vector-machine}{%
\subsubsection{The Support Vector
Machine}\label{the-support-vector-machine}}

    \begin{itemize}
\item
  It can be shown that:

  \begin{itemize}
  \tightlist
  \item
    the linear support vector classifier is a model of the form
    \[f(x) = \beta_0 + \sum_{i = 1}^n \alpha_i \langle x, x_i\rangle \]
  \item
    the parameter estimates \(\hat{\alpha}_i, \hat{\beta}_0\) can be
    computed from the \(\binom{n}{2}\) inner products
    \(\langle x, x_i \rangle\)
  \end{itemize}
\item
  The support vector machine is a model of the form
  \[f(x) = \beta_0 + \sum_{i = 1}^n \alpha_i K(x, x_i) \] where \(K\) is
  a \textbf{\emph{kernel function}} 81 
\item
  Popular kernels 82 are

  \begin{itemize}
  \tightlist
  \item
    The \textbf{\emph{polynomial kernel}}
    \[K(x_i, x_i') = (1 + x_i^\top x_i')^d\]
  \item
    The \textbf{\emph{radial kernel}}
    \[K(x_i, x_i') = \exp(-\gamma\,||x_i - x_i'||^2)\]
  \end{itemize}
\end{itemize}

    \hypertarget{svms-with-more-than-two-classes}{%
\subsection{SVMs with More than Two
Classes}\label{svms-with-more-than-two-classes}}

    \hypertarget{one-versus-one-classification}{%
\subsubsection{One-Versus-One
Classification}\label{one-versus-one-classification}}

    This approach works as follows: 1. Fit \(\binom{K}{2}\) SVMs, one for
each pair of classes \(k,k'\) encoded as \(\pm 1\), respectively. 2. For
each observation \(x\), classify using each of the predictors in 1, and
let \(N_k\) be the number of times \(x\) was assigned to class \(k\). 3.
Predict \[ \hat{f}(x) = \underset{k}{\text{argmax}}\, N_k\]

    \hypertarget{one-versus-all-classification}{%
\subsubsection{One-Versus-All
Classification}\label{one-versus-all-classification}}

    This approach works as follows: 1. Fit \(K\) SVMs, comparing each class
\(k\) to other \(K-1\) classes, encoded as \(\pm 1\), respectively. Let
\(\beta_k (\beta_{0k}, \dots, \beta_{pk})\) be resulting parameters. 2.
Predict \[\hat{f}(x) = \underset{k}{\text{argmax}}\, \beta_k^\top x\]

    \hypertarget{relationship-to-logistic-regression}{%
\subsection{Relationship to Logistic
Regression}\label{relationship-to-logistic-regression}}

    \begin{itemize}
\tightlist
\item
  The optimization problem leading to the support vector classifier can
  be rewritten as
  \[\underset{\beta}{\text{argmin}}\left(\sum_{i = 1}^n \max\{0, 1 - y_i(\beta^\top x_i)\} + \lambda\,||\beta||^2\right)\]
  where \$\lambda \geqslant 0 \$ is a tuning parameter 83 .
\item
  The hinge loss 84 is very similar to the logistic regression loss, so
  both methods tend to give similar results. However, SVMs tend to
  perform better when the classes are well separated, while logistic
  regression tends to perform better when they are not.
\end{itemize}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{footnotes}{%
\subsection{Footnotes}\label{footnotes}}

    \hypertarget{foot78}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{77}
\tightlist
\item
  The constraint \(|| \boldsymbol{\beta} || = 1\) ensures that the
  perpendicular distance \(||x_i - P x_i||\) is given by
  \(y_i(\beta^\top x_i)\). ↩
\end{enumerate}

\hypertarget{foot79}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{78}
\tightlist
\item
  Sometimes the maximal margin and support vector classifiers are called
  ``hard margin'' and ``soft margin'' support vector classifiers,
  respectively. ↩
\end{enumerate}

\hypertarget{foot80}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{79}
\tightlist
\item
  For each \(i\), if \(\epsilon_i = 0\) the \(i\)-th observation is on
  the correct side of the margin. If \(\epsilon_i > 0\) then it is on
  the wrong side of the margin, and if \(\epsilon_i > 1\) then it is on
  the wrong side of the hyperplane. The parameter \(C\) is a ``margin
  violation tolerance'' -- it bounds the \(\epsilon_i\) and thus the
  number/size of margin violations. Greater \(C\) implies greater
  tolerance. The case \(C = 0\) is the maximal margin hyperplane. ↩
\end{enumerate}

\leavevmode\hypertarget{foot81}{}%
81.In this context a kernel function is a positive-definite kernel .
Among other things, it is a generalization of an inner product (every
inner product \(\langle x, y \rangle\) is a kernel function), and is one
way of quantifying similarity between points.\textbackslash{}

In the context of statistical and machine learning, a kernel method is
one which makes use of the ``kernel trick''. The kernel function
\(K(x_i, x_i')\) encodes the similarity of the observations
\(x_i, x_i'\) in a transformed feature space, but it is more
computationally efficient to compute the \(\binom{n}{k}\) kernels
themselves than to transform the data. The kernel fits a support vector
classifier (hence a linear classification boundary) in the transformed
feature space, which corresponds to a non-linear boundary in the
original feature space.

\hypertarget{foot82}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{81}
\tightlist
\item
  The polynomial kernel is effectively the inner product on the space of
  \(d\)-degree polynomials in the features \(X_j\). The radial kernel is
  a similarity measure in an infinite dimensional feature space.
\end{enumerate}

\hypertarget{foot83}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{82}
\tightlist
\item
  This is another instance of a general form of a ``regularized loss''
  or ``loss + penalty''
\end{enumerate}

\[\underset{\beta}{\text{argmin}}L(\mathbf{X}, \mathbf{y}, \beta) + \lambda P(\beta)\]
where the loss function \(L(\mathbf{X}, \mathbf{y}, \beta)\) quantifies
how well the parameter model with parameter \(\beta\) fits the data
\((\mathbf{X}, \mathbf{y})\), and \(P(\beta)\) is a penalty function
controlled by \(\lambda\).

\hypertarget{foot84}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{83}
\tightlist
\item
  In this case
  \[L(\mathbf{X}, \mathbf{y}, \beta) = \sum_{i = 1}^n \{0, 1 - y_i(\beta^\top x_i\}\]
\end{enumerate}

is called the \textbf{\emph{hinge loss}}.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
