<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      3. Linear Regression &middot; islr
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/islr/public/css/poole.css">
  <link rel="stylesheet" href="/islr/public/css/syntax.css">
  <link rel="stylesheet" href="/islr/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/islr/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/islr/public/favicon.ico">
  --->

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!--- KaTeX --->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
      onload="renderMathInElement(document.body);"></script>
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/islr/">Home</a>

    
    
      
    
      
        
      
    
      
    
      
        
          
          <a class="sidebar-nav-item" href="/islr/ch02/">2. Statistical Learning</a>
        
      
    
      
        
      
    
      
        
          
          <a class="sidebar-nav-item" href="/islr/ch03/">3. Linear Regression</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          
          <a class="sidebar-nav-item" href="/islr/ch04/">4. Logistic Regression</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          
          <a class="sidebar-nav-item" href="/islr/ch05/">5. Resampling Methods</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          
          <a class="sidebar-nav-item" href="/islr/ch06/">6. Linear Model Selection and Regularization</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          
          <a class="sidebar-nav-item" href="/islr/ch07/">7. Moving Beyond Linearity</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          
          <a class="sidebar-nav-item" href="/islr/ch08/">8. Tree-Based Methods</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          
          <a class="sidebar-nav-item" href="/islr/ch09/">9. Support Vector Machines</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          
          <a class="sidebar-nav-item" href="/islr/ch10/">10. Unsupervised Learning</a>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
    
      
    
      
    
      
    

  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2019.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/islr/" title="Home">islr</a>
            <small>notes and exercises from An Introduction to Statistical Learning</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="page">
  <h1 class="page-title">3. Linear Regression</h1>
  	<h1 id="exercise-9-multiple-regression-of-mpg-on-numerical-features-in-auto">Exercise 9: Multiple regression of <code class="highlighter-rouge">mpg</code> on numerical features in <code class="highlighter-rouge">auto</code></h1>

<div class="toc">
  <ul class="toc-item"><li><span><a href="#preparing-the-dataset" data-toc-modified-id="Preparing-the-dataset-1">Preparing the dataset</a></span></li><li><span><a href="#a.-Scatterplot-matrix-of-auto" data-toc-modified-id="a.-Scatterplot-matrix-of-auto-2">a. Scatterplot matrix of <code>auto</code></a></span></li><li><span><a href="#b-correlation-matrix-of-auto" data-toc-modified-id="b.-Correlation-matrix-of-auto-3">b. Correlation matrix of <code>auto</code></a></span></li><li><span><a href="#c-fitting-the-model" data-toc-modified-id="c.-Fitting-the-model-4">c. Fitting the model</a></span><ul class="toc-item"><li><ul class="toc-item"><li><span><a href="#i-is-there-a-relationship-between-the-predictors-and-the-mpg?" data-toc-modified-id="i.-Is-there-a-relationship-between-the-predictors-and-the-mpg?-4.0.1">i. Is there a relationship between the predictors and the <code>mpg</code>?</a></span></li><li><span><a href="#ii-which-predictors-appear-to-have-a-statistically-significant-relationship-to-the-response?" data-toc-modified-id="ii.-Which-predictors-appear-to-have-a-statistically-significant-relationship-to-the-response?-4.0.2">ii. Which predictors appear to have a statistically significant relationship to the response?</a></span></li><li><span><a href="#iii-what-does-the-coefficient-for-the-year-variable-suggest?" data-toc-modified-id="iii.-What-does-the-coefficient-for-the-year-variable-suggest?-4.0.3">iii. What does the coefficient for the year variable suggest?</a></span></li></ul></li></ul></li><li><span><a href="#d-diagnostic-plots" data-toc-modified-id="d.-Diagnostic-plots-5">d. Diagnostic plots</a></span><ul class="toc-item"><li><span><a href="#standardized-residuals-vs-fitted-value" data-toc-modified-id="Standardized-residuals-vs-fitted-value-5.1">Standardized residuals vs fitted value</a></span></li><li><span><a href="#standardized-residuals-QQ-plot" data-toc-modified-id="Standardized-residuals-QQ-plot-5.2">Standardized residuals QQ-plot</a></span></li><li><span><a href="#Scale-location-plot" data-toc-modified-id="Scale-location-plot-5.3">Scale-location plot</a></span></li><li><span><a href="#influence-plot" data-toc-modified-id="Influence-Plot-5.4">Influence Plot</a></span></li></ul></li><li><span><a href="#e-interaction-effects" data-toc-modified-id="e.-Interaction-effects-6">e. Interaction effects</a></span></li><li><span><a href="#f-variable-transformations" data-toc-modified-id="f.-Variable-transformations-7">f. Variable transformations</a></span><ul class="toc-item"><li><span><a href="#the-log-model" data-toc-modified-id="The-log-model-7.1">The $\log(X)$ model</a></span></li><li><span><a href="#the-square-root-model" data-toc-modified-id="The-square-root-model-7.2">The $\sqrt{X}$ model</a></span></li><li><span><a href="#the-squared-model" data-toc-modified-id="The-squared-model-7.3">The $X^2$ model</a></span></li></ul></li></ul>
</div>

<h2 id="preparing-the-dataset">Preparing the dataset</h2>

<p>Import pandas, load the <code class="highlighter-rouge">Auto</code> dataset, and inspect</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">auto</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'../../datasets/Auto.csv'</span><span class="p">)</span>
<span class="n">auto</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
  <style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-aligned: middle;
    }

    .dataframe tbody tr th {
        vertical-aligned: top;
    }

    .dataframe thead th {
        text-aligned: right;
    }
</style>

  <table border="1" class="dataframe">
  <thead>
    <tr style="text-aligned: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>mpg</th>
      <th>cylinders</th>
      <th>displacement</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>year</th>
      <th>origin</th>
      <th>name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>18.0</td>
      <td>8</td>
      <td>307.0</td>
      <td>130</td>
      <td>3504</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>chevrolet chevelle malibu</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>15.0</td>
      <td>8</td>
      <td>350.0</td>
      <td>165</td>
      <td>3693</td>
      <td>11.5</td>
      <td>70</td>
      <td>1</td>
      <td>buick skylark 320</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>18.0</td>
      <td>8</td>
      <td>318.0</td>
      <td>150</td>
      <td>3436</td>
      <td>11.0</td>
      <td>70</td>
      <td>1</td>
      <td>plymouth satellite</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>16.0</td>
      <td>8</td>
      <td>304.0</td>
      <td>150</td>
      <td>3433</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>amc rebel sst</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>17.0</td>
      <td>8</td>
      <td>302.0</td>
      <td>140</td>
      <td>3449</td>
      <td>10.5</td>
      <td>70</td>
      <td>1</td>
      <td>ford torino</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">auto</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 392 entries, 0 to 391
Data columns (total 10 columns):
Unnamed: 0      392 non-null int64
mpg             392 non-null float64
cylinders       392 non-null int64
displacement    392 non-null float64
horsepower      392 non-null int64
weight          392 non-null int64
acceleration    392 non-null float64
year            392 non-null int64
origin          392 non-null int64
name            392 non-null object
dtypes: float64(3), int64(6), object(1)
memory usage: 30.7+ KB
</code></pre></div></div>

<p>There are missing values represented by <code class="highlighter-rouge">'?'</code> in  <code class="highlighter-rouge">horsepower</code>. We’ll impute these by using mean values for the <code class="highlighter-rouge">cylinders</code> class</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># replace `?` with nans</span>
<span class="n">auto</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s">'horsepower'</span><span class="p">]</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'?'</span> <span class="k">else</span> <span class="n">x</span><span class="p">)</span>

<span class="c"># cast horsepower to numeric dtype</span>
<span class="n">auto</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s">'horsepower'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">auto</span><span class="o">.</span><span class="n">horsepower</span><span class="p">)</span>

<span class="c"># now impute values</span>
<span class="n">auto</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s">'horsepower'</span><span class="p">]</span> <span class="o">=</span> <span class="n">auto</span><span class="o">.</span><span class="n">horsepower</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">auto</span><span class="o">.</span><span class="n">horsepower</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">auto</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 392 entries, 0 to 391
Data columns (total 10 columns):
Unnamed: 0      392 non-null int64
mpg             392 non-null float64
cylinders       392 non-null int64
displacement    392 non-null float64
horsepower      392 non-null int64
weight          392 non-null int64
acceleration    392 non-null float64
year            392 non-null int64
origin          392 non-null int64
name            392 non-null object
dtypes: float64(3), int64(6), object(1)
memory usage: 30.7+ KB
</code></pre></div></div>

<h2 id="a-scatterplot-matrix-of-auto">a. Scatterplot matrix of <code class="highlighter-rouge">auto</code></h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># setup</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">'seaborn-white'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s">'white'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">auto</span><span class="o">.</span><span class="n">dropna</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="/islr/assets/images/ch03_exercise_09_10_1.png" alt="png" /></p>

<h2 id="b-correlation-matrix-of-auto">b. Correlation matrix of <code class="highlighter-rouge">auto</code></h2>

<p>Compute the correlation matrix of the numerical variables</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">auto</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</code></pre></div></div>

<div>
  <style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-aligned: middle;
    }

    .dataframe tbody tr th {
        vertical-aligned: top;
    }

    .dataframe thead th {
        text-aligned: right;
    }
</style>

  <table border="1" class="dataframe">
  <thead>
    <tr style="text-aligned: right;">
      <th></th>
      <th>mpg</th>
      <th>cylinders</th>
      <th>displacement</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>year</th>
      <th>origin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mpg</th>
      <td>1.000000</td>
      <td>-0.776260</td>
      <td>-0.804443</td>
      <td>-0.776230</td>
      <td>-0.831739</td>
      <td>0.422297</td>
      <td>0.581469</td>
      <td>0.563698</td>
    </tr>
    <tr>
      <th>cylinders</th>
      <td>-0.776260</td>
      <td>1.000000</td>
      <td>0.950920</td>
      <td>0.843640</td>
      <td>0.897017</td>
      <td>-0.504061</td>
      <td>-0.346717</td>
      <td>-0.564972</td>
    </tr>
    <tr>
      <th>displacement</th>
      <td>-0.804443</td>
      <td>0.950920</td>
      <td>1.000000</td>
      <td>0.897584</td>
      <td>0.933104</td>
      <td>-0.544162</td>
      <td>-0.369804</td>
      <td>-0.610664</td>
    </tr>
    <tr>
      <th>horsepower</th>
      <td>-0.776230</td>
      <td>0.843640</td>
      <td>0.897584</td>
      <td>1.000000</td>
      <td>0.864320</td>
      <td>-0.688223</td>
      <td>-0.415617</td>
      <td>-0.451925</td>
    </tr>
    <tr>
      <th>weight</th>
      <td>-0.831739</td>
      <td>0.897017</td>
      <td>0.933104</td>
      <td>0.864320</td>
      <td>1.000000</td>
      <td>-0.419502</td>
      <td>-0.307900</td>
      <td>-0.581265</td>
    </tr>
    <tr>
      <th>acceleration</th>
      <td>0.422297</td>
      <td>-0.504061</td>
      <td>-0.544162</td>
      <td>-0.688223</td>
      <td>-0.419502</td>
      <td>1.000000</td>
      <td>0.282901</td>
      <td>0.210084</td>
    </tr>
    <tr>
      <th>year</th>
      <td>0.581469</td>
      <td>-0.346717</td>
      <td>-0.369804</td>
      <td>-0.415617</td>
      <td>-0.307900</td>
      <td>0.282901</td>
      <td>1.000000</td>
      <td>0.184314</td>
    </tr>
    <tr>
      <th>origin</th>
      <td>0.563698</td>
      <td>-0.564972</td>
      <td>-0.610664</td>
      <td>-0.451925</td>
      <td>-0.581265</td>
      <td>0.210084</td>
      <td>0.184314</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="c-fitting-the-model">c. Fitting the model</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>

<span class="c"># drop non-numerical columns and rows with null entries</span>
<span class="n">model_df</span> <span class="o">=</span> <span class="n">auto</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'name'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">model_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'mpg'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">model_df</span><span class="o">.</span><span class="n">mpg</span>

<span class="c"># add constant</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c"># create and fit model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c"># show results summary</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.822</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.819</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   256.4</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 28 Oct 2018</td> <th>  Prob (F-statistic):</th> <td>1.89e-141</td>
</tr>
<tr>
  <th>Time:</th>                 <td>19:28:06</td>     <th>  Log-Likelihood:    </th> <td> -1037.2</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>   397</td>      <th>  AIC:               </th> <td>   2090.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   389</td>      <th>  BIC:               </th> <td>   2122.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P&gt;|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>        <td>  -18.0900</td> <td>    4.629</td> <td>   -3.908</td> <td> 0.000</td> <td>  -27.191</td> <td>   -8.989</td>
</tr>
<tr>
  <th>cylinders</th>    <td>   -0.4560</td> <td>    0.322</td> <td>   -1.414</td> <td> 0.158</td> <td>   -1.090</td> <td>    0.178</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.0196</td> <td>    0.008</td> <td>    2.608</td> <td> 0.009</td> <td>    0.005</td> <td>    0.034</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.0136</td> <td>    0.014</td> <td>   -0.993</td> <td> 0.321</td> <td>   -0.040</td> <td>    0.013</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.0066</td> <td>    0.001</td> <td>  -10.304</td> <td> 0.000</td> <td>   -0.008</td> <td>   -0.005</td>
</tr>
<tr>
  <th>acceleration</th> <td>    0.0998</td> <td>    0.098</td> <td>    1.021</td> <td> 0.308</td> <td>   -0.092</td> <td>    0.292</td>
</tr>
<tr>
  <th>year</th>         <td>    0.7587</td> <td>    0.051</td> <td>   14.969</td> <td> 0.000</td> <td>    0.659</td> <td>    0.858</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.4199</td> <td>    0.277</td> <td>    5.132</td> <td> 0.000</td> <td>    0.876</td> <td>    1.964</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>30.088</td> <th>  Durbin-Watson:     </th> <td>   1.294</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  48.301</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.511</td> <th>  Prob(JB):          </th> <td>3.25e-11</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.370</td> <th>  Cond. No.          </th> <td>8.58e+04</td>
</tr>
</table>
<p><br /><br />Warnings:<br />[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br />[2] The condition number is large, 8.58e+04. This might indicate that there are<br />strong multicollinearity or other numerical problems.</p>

<h4 id="i-is-there-a-relationship-between-the-predictors-and-the-mpg">i. Is there a relationship between the predictors and the <code class="highlighter-rouge">mpg</code>?</h4>

<p>This question is answered by the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>-value of the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span></span></span></span>-statistic</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">f_pvalue</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.8936359873496686e-141
</code></pre></div></div>

<p>This is effectively zero, so the answer is yes</p>

<h4 id="ii-which-predictors-appear-to-have-a-statistically-significant-relationship-to-the-response">ii. Which predictors appear to have a statistically significant relationship to the response?</h4>

<p>This is answered by the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>-values of the individual predictors</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">pvalues</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>const           1.097017e-04
cylinders       1.580259e-01
displacement    9.455004e-03
horsepower      3.212038e-01
weight          3.578587e-22
acceleration    3.077592e-01
year            2.502539e-40
origin          4.530034e-07
dtype: float64
</code></pre></div></div>

<p>A common cutoff is a <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>-value of 0.05, so by this standard, the predictors with a statistically significant relationship to <code class="highlighter-rouge">mpg</code> are</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">is_stat_sig</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pvalues</span> <span class="o">&lt;</span> <span class="mf">0.05</span>
<span class="n">model</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="n">is_stat_sig</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>const           1.097017e-04
displacement    9.455004e-03
weight          3.578587e-22
year            2.502539e-40
origin          4.530034e-07
dtype: float64
</code></pre></div></div>

<p>And those which do not are</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="o">~</span> <span class="n">is_stat_sig</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cylinders       0.158026
horsepower      0.321204
acceleration    0.307759
dtype: float64
</code></pre></div></div>

<p>This is surprising, since we found a statistically significant relationship between <code class="highlighter-rouge">horsepower</code> and <code class="highlighter-rouge">mpg</code> in <a href="/islr/ch03/ch03_exercises/ch03_exercise_08">exercise 8</a>.</p>

<h4 id="iii-what-does-the-coefficient-for-the-year-variable-suggest">iii. What does the coefficient for the year variable suggest?</h4>

<p>That fuel efficiency has been improving over time</p>

<h2 id="d-diagnostic-plots">d. Diagnostic plots</h2>

<p>First we assemble the results in a dataframe and clean up a bit</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># get full prediction results</span>
<span class="n">pred_df</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_prediction</span><span class="p">()</span><span class="o">.</span><span class="n">summary_frame</span><span class="p">()</span>

<span class="c"># rename columns to avoid `mean` name conflicts and other confusions</span>
<span class="n">new_names</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">pred_df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="s">'mean'</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="n">new_names</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'mean'</span><span class="p">,</span> <span class="s">'mpg_pred'</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s">'obs_ci'</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="n">new_names</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'obs_ci'</span><span class="p">,</span> <span class="s">'mpg_pred_pi'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_names</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">name</span>
<span class="n">pred_df</span> <span class="o">=</span> <span class="n">pred_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">new_names</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s">'columns'</span><span class="p">)</span>

<span class="c"># concat into final df</span>
<span class="n">model_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">model_df</span><span class="p">,</span> <span class="n">pred_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
  <style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-aligned: middle;
    }

    .dataframe tbody tr th {
        vertical-aligned: top;
    }

    .dataframe thead th {
        text-aligned: right;
    }
</style>

  <table border="1" class="dataframe">
  <thead>
    <tr style="text-aligned: right;">
      <th></th>
      <th>mpg</th>
      <th>cylinders</th>
      <th>displacement</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>year</th>
      <th>origin</th>
      <th>mpg_pred</th>
      <th>mpg_pred_se</th>
      <th>mpg_pred_ci_lower</th>
      <th>mpg_pred_ci_upper</th>
      <th>mpg_pred_pi_lower</th>
      <th>mpg_pred_pi_upper</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18.0</td>
      <td>8</td>
      <td>307.0</td>
      <td>130.0</td>
      <td>3504</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>14.966498</td>
      <td>0.506952</td>
      <td>13.969789</td>
      <td>15.963208</td>
      <td>8.338758</td>
      <td>21.594239</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15.0</td>
      <td>8</td>
      <td>350.0</td>
      <td>165.0</td>
      <td>3693</td>
      <td>11.5</td>
      <td>70</td>
      <td>1</td>
      <td>14.028743</td>
      <td>0.446127</td>
      <td>13.151621</td>
      <td>14.905865</td>
      <td>7.417930</td>
      <td>20.639557</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18.0</td>
      <td>8</td>
      <td>318.0</td>
      <td>150.0</td>
      <td>3436</td>
      <td>11.0</td>
      <td>70</td>
      <td>1</td>
      <td>15.262507</td>
      <td>0.487309</td>
      <td>14.304418</td>
      <td>16.220595</td>
      <td>8.640464</td>
      <td>21.884549</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.0</td>
      <td>8</td>
      <td>304.0</td>
      <td>150.0</td>
      <td>3433</td>
      <td>12.0</td>
      <td>70</td>
      <td>1</td>
      <td>15.107684</td>
      <td>0.493468</td>
      <td>14.137487</td>
      <td>16.077882</td>
      <td>8.483879</td>
      <td>21.731490</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.0</td>
      <td>8</td>
      <td>302.0</td>
      <td>140.0</td>
      <td>3449</td>
      <td>10.5</td>
      <td>70</td>
      <td>1</td>
      <td>14.948273</td>
      <td>0.535264</td>
      <td>13.895900</td>
      <td>16.000646</td>
      <td>8.311933</td>
      <td>21.584612</td>
    </tr>
  </tbody>
</table>
</div>

<p>Now we plot the 4 diagnostic plots returned by R’s <code class="highlighter-rouge">lm()</code> function (see [<a href="/islr/ch03/ch03_exercises/ch03_exercise_08">exercise 8</a>)</p>

<h3 id="standardized-residuals-vs-fitted-value">Standardized residuals vs fitted value</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># add residuals to df</span>
<span class="n">model_df</span><span class="p">[</span><span class="s">'resid'</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">resid</span>


<span class="c"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'standardized resid'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">model_df</span><span class="o">.</span><span class="n">mpg_pred</span><span class="p">,</span> <span class="n">model_df</span><span class="o">.</span><span class="n">resid</span><span class="o">/</span><span class="n">model_df</span><span class="o">.</span><span class="n">resid</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">lowess</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
            <span class="n">line_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'color'</span><span class="p">:</span><span class="s">'r'</span><span class="p">,</span> <span class="s">'lw'</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span>
            <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'facecolors'</span><span class="p">:</span><span class="s">'grey'</span><span class="p">,</span> <span class="s">'edgecolors'</span><span class="p">:</span><span class="s">'grey'</span><span class="p">,</span> <span class="s">'alpha'</span><span class="p">:</span><span class="mf">0.4</span><span class="p">})</span>
</code></pre></div></div>

<p><img src="/islr/assets/images/ch03_exercise_09_31_1.png" alt="png" /></p>

<h3 id="standardized-residuals-qq-plot">Standardized residuals QQ-plot</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">model_df</span><span class="o">.</span><span class="n">resid</span><span class="o">/</span><span class="n">model_df</span><span class="o">.</span><span class="n">resid</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s">''</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'studentized resid quantiles'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'standard normal quantiles'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/islr/assets/images/ch03_exercise_09_33_1.png" alt="png" /></p>

<h3 id="scale-location-plot">Scale-location plot</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'√|standardized resid|'</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">model_df</span><span class="o">.</span><span class="n">mpg_pred</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">model_df</span><span class="o">.</span><span class="n">resid</span><span class="o">/</span><span class="n">model_df</span><span class="o">.</span><span class="n">resid</span><span class="o">.</span><span class="n">std</span><span class="p">())),</span> <span class="n">lowess</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
            <span class="n">line_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'color'</span><span class="p">:</span><span class="s">'r'</span><span class="p">,</span> <span class="s">'lw'</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span>
            <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'facecolors'</span><span class="p">:</span><span class="s">'grey'</span><span class="p">,</span> <span class="s">'edgecolors'</span><span class="p">:</span><span class="s">'grey'</span><span class="p">,</span> <span class="s">'alpha'</span><span class="p">:</span><span class="mf">0.4</span><span class="p">})</span>
</code></pre></div></div>

<p><img src="/islr/assets/images/ch03_exercise_09_35_1.png" alt="png" /></p>

<h3 id="influence-plot">Influence Plot</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># influence plot</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">get_influence</span><span class="p">()</span><span class="o">.</span><span class="n">hat_matrix_diag</span><span class="p">,</span> <span class="n">model_df</span><span class="o">.</span><span class="n">resid</span><span class="o">/</span><span class="n">model_df</span><span class="o">.</span><span class="n">resid</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> 
            <span class="n">lowess</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
            <span class="n">line_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'color'</span><span class="p">:</span><span class="s">'r'</span><span class="p">,</span> <span class="s">'lw'</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span>
            <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'facecolors'</span><span class="p">:</span><span class="s">'grey'</span><span class="p">,</span> <span class="s">'edgecolors'</span><span class="p">:</span><span class="s">'grey'</span><span class="p">,</span> <span class="s">'alpha'</span><span class="p">:</span><span class="mf">0.4</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'leverage'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'studentized resid'</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">axes</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">x</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/islr/assets/images/ch03_exercise_09_37_1.png" alt="png" /></p>

<p>From these diagnostic plots we conclude</p>

<ul>
  <li>There is non-linearity in the data</li>
  <li>There are a handful of outliers (studentized residual <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>⩾</mo></mrow><annotation encoding="application/x-tex">\geqslant</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7733399999999999em;vertical-align:-0.13667em;"></span><span class="mrel amsrm">⩾</span></span></span></span> 3)</li>
  <li>The normality assumption is appropriate</li>
  <li>The data shows heteroscedasticity</li>
  <li>There are no high influence points</li>
</ul>

<h2 id="e-interaction-effects">e. Interaction effects</h2>

<p>We are told to use the <code class="highlighter-rouge">*</code> and <code class="highlighter-rouge">~</code> R operators to investigate interaction effects. Thankfully <a href="http://www.statsmodels.org/devel/example_formulas.html">statmodels has support for these</a>.</p>

<p>To use <code class="highlighter-rouge">:
</code>, we will fit a model consisting of only pairwise interaction terms <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><msub><mi>X</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">X_iX_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">itertools</span> <span class="k">as</span> <span class="n">it</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="n">smf</span>


<span class="c"># generate formula for interaction terms </span>
<span class="n">names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">auto</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'name'</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'mpg'</span><span class="p">))</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">names</span><span class="p">))</span>
<span class="n">terms</span>  <span class="o">=</span> <span class="p">[</span><span class="n">name1</span> <span class="o">+</span> <span class="s">' : '</span> <span class="o">+</span> <span class="n">name2</span> <span class="k">for</span> <span class="p">(</span><span class="n">name1</span><span class="p">,</span> <span class="n">name2</span><span class="p">)</span> <span class="ow">in</span> <span class="n">pairs</span> <span class="k">if</span> <span class="n">name1</span> <span class="o">!=</span> <span class="n">name2</span><span class="p">]</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s">'mpg ~ '</span>

<span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">terms</span><span class="p">:</span>
    <span class="n">formula</span> <span class="o">+=</span> <span class="n">term</span> <span class="o">+</span> <span class="s">' + '</span>
<span class="n">formula</span> <span class="o">=</span> <span class="n">formula</span><span class="p">[:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
<span class="n">formula</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'mpg ~ cylinders : displacement + cylinders : horsepower + cylinders : weight + cylinders : acceleration + cylinders : year + cylinders : origin + displacement : cylinders + displacement : horsepower + displacement : weight + displacement : acceleration + displacement : year + displacement : origin + horsepower : cylinders + horsepower : displacement + horsepower : weight + horsepower : acceleration + horsepower : year + horsepower : origin + weight : cylinders + weight : displacement + weight : horsepower + weight : acceleration + weight : year + weight : origin + acceleration : cylinders + acceleration : displacement + acceleration : horsepower + acceleration : weight + acceleration : year + acceleration : origin + year : cylinders + year : displacement + year : horsepower + year : weight + year : acceleration + year : origin + origin : cylinders + origin : displacement + origin : horsepower + origin : weight + origin : acceleration + origin : year'
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># fit a regression model with only interaction terms</span>
<span class="n">pair_int_model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">auto</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</code></pre></div></div>

<p>And find the statisitcally significant interactions</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># show interactions with p value less than 0.05</span>
<span class="n">pair_int_model</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="n">pair_int_model</span><span class="o">.</span><span class="n">pvalues</span> <span class="o">&lt;</span> <span class="mf">5e-2</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Intercept                    0.005821
cylinders:year               0.014595
displacement:acceleration    0.010101
displacement:year            0.000036
displacement:origin          0.015855
weight:acceleration          0.005177
acceleration:year            0.000007
year:origin                  0.045881
dtype: float64
</code></pre></div></div>

<p>Now to use <code class="highlighter-rouge">+</code> we fit a model consisting of all features <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">X_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and all possible interactions</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># generate formula for interaction terms </span>
<span class="n">names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">auto</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'name'</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'mpg'</span><span class="p">))</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s">'mpg ~ '</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
    <span class="n">formula</span> <span class="o">+=</span> <span class="n">name</span> <span class="o">+</span> <span class="s">'*'</span>
<span class="n">formula</span> <span class="o">=</span> <span class="n">formula</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">formula</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'mpg ~ cylinders*displacement*horsepower*weight*acceleration*year*origin'
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># fit a regression model with all features and all possible interaction terms</span>
<span class="n">full_int_model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">auto</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</code></pre></div></div>

<p>Finally, we find the statistically significant terms</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">full_int_model</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="n">full_int_model</span><span class="o">.</span><span class="n">pvalues</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Series([], dtype: float64)
</code></pre></div></div>

<p>In this case, including all possible interactions has led to none of them being statistically significant, even the pairwise interactions.</p>

<h2 id="f-variable-transformations">f. Variable transformations</h2>

<p>We’ll try the suggested variable transformations <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msqrt><mi>X</mi></msqrt><mo separator="true">,</mo><msup><mi>X</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\log(X), \sqrt{X}, X^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.176665em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9266650000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span><span style="top:-2.886665em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5, -10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8, -50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0, 35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5, -221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467 s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422 s-65,47,-65,47z M834 80H400000v40H845z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.11333499999999996em;"><span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># drop constant before transformation, else const for log(X) will be zero</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'const'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>

<span class="c"># transform data</span>
<span class="n">log_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">sqrt_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_sq</span> <span class="o">=</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span>

<span class="c"># fit models with constants</span>
<span class="n">log_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">log_X</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">sqrt_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">sqrt_X</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">sq_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_sq</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</code></pre></div></div>

<p>Now we’ll look at each of these models individually:</p>

<h3 id="the-log-model">The log model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">log_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.848</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.845</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   310.3</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 28 Oct 2018</td> <th>  Prob (F-statistic):</th> <td>6.92e-155</td>
</tr>
<tr>
  <th>Time:</th>                 <td>19:28:07</td>     <th>  Log-Likelihood:    </th> <td> -1005.5</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>   397</td>      <th>  AIC:               </th> <td>   2027.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   389</td>      <th>  BIC:               </th> <td>   2059.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P&gt;|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>        <td>  -67.0838</td> <td>   17.433</td> <td>   -3.848</td> <td> 0.000</td> <td> -101.358</td> <td>  -32.810</td>
</tr>
<tr>
  <th>cylinders</th>    <td>    1.8114</td> <td>    1.658</td> <td>    1.093</td> <td> 0.275</td> <td>   -1.448</td> <td>    5.070</td>
</tr>
<tr>
  <th>displacement</th> <td>   -1.0935</td> <td>    1.540</td> <td>   -0.710</td> <td> 0.478</td> <td>   -4.121</td> <td>    1.934</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -6.2631</td> <td>    1.528</td> <td>   -4.100</td> <td> 0.000</td> <td>   -9.267</td> <td>   -3.259</td>
</tr>
<tr>
  <th>weight</th>       <td>  -13.4966</td> <td>    2.185</td> <td>   -6.178</td> <td> 0.000</td> <td>  -17.792</td> <td>   -9.201</td>
</tr>
<tr>
  <th>acceleration</th> <td>   -4.3687</td> <td>    1.577</td> <td>   -2.770</td> <td> 0.006</td> <td>   -7.469</td> <td>   -1.268</td>
</tr>
<tr>
  <th>year</th>         <td>   55.5963</td> <td>    3.540</td> <td>   15.704</td> <td> 0.000</td> <td>   48.636</td> <td>   62.557</td>
</tr>
<tr>
  <th>origin</th>       <td>    1.5763</td> <td>    0.506</td> <td>    3.118</td> <td> 0.002</td> <td>    0.582</td> <td>    2.570</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>39.413</td> <th>  Durbin-Watson:     </th> <td>   1.381</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  76.214</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.576</td> <th>  Prob(JB):          </th> <td>2.82e-17</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.812</td> <th>  Cond. No.          </th> <td>1.36e+03</td>
</tr>
</table>
<p><br /><br />Warnings:<br />[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br />[2] The condition number is large, 1.36e+03. This might indicate that there are<br />strong multicollinearity or other numerical problems.</p>

<p>Very large <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> and very low <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>-value for the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span></span></span></span>-statistic suggest this is a useful model. Interestingly, this model gives very large <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>-values for the features <code class="highlighter-rouge">cylinders</code> and <code class="highlighter-rouge">displacement</code>.</p>

<p>The statistically significant features of the the original and log models and their p-values</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stat_sig_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="n">is_stat_sig</span><span class="p">],</span> <span class="n">log_model</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="n">is_stat_sig</span><span class="p">]],</span> <span class="n">join</span><span class="o">=</span><span class="s">'outer'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">stat_sig_df</span> <span class="o">=</span> <span class="n">stat_sig_df</span><span class="o">.</span><span class="n">rename</span><span class="p">({</span><span class="mi">0</span> <span class="p">:</span> <span class="s">'model_pval'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s">'log_model_pval'</span><span class="p">},</span> <span class="n">axis</span><span class="o">=</span><span class="s">'columns'</span><span class="p">)</span>
<span class="n">stat_sig_df</span>
</code></pre></div></div>

<div>
  <style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-aligned: middle;
    }

    .dataframe tbody tr th {
        vertical-aligned: top;
    }

    .dataframe thead th {
        text-aligned: right;
    }
</style>

  <table border="1" class="dataframe">
  <thead>
    <tr style="text-aligned: right;">
      <th></th>
      <th>model_pval</th>
      <th>log_model_pval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>const</th>
      <td>1.097017e-04</td>
      <td>1.390585e-04</td>
    </tr>
    <tr>
      <th>displacement</th>
      <td>9.455004e-03</td>
      <td>4.780250e-01</td>
    </tr>
    <tr>
      <th>weight</th>
      <td>3.578587e-22</td>
      <td>1.641584e-09</td>
    </tr>
    <tr>
      <th>year</th>
      <td>2.502539e-40</td>
      <td>2.150054e-43</td>
    </tr>
    <tr>
      <th>origin</th>
      <td>4.530034e-07</td>
      <td>1.958604e-03</td>
    </tr>
  </tbody>
</table>
</div>

<p>The insignificant features and p-values are</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stat_sig_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="o">~</span> <span class="n">is_stat_sig</span><span class="p">],</span> <span class="n">log_model</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="o">~</span> <span class="n">is_stat_sig</span><span class="p">]],</span> <span class="n">join</span><span class="o">=</span><span class="s">'outer'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">stat_sig_df</span> <span class="o">=</span> <span class="n">stat_sig_df</span><span class="o">.</span><span class="n">rename</span><span class="p">({</span><span class="mi">0</span> <span class="p">:</span> <span class="s">'model_pval'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s">'log_model_pval'</span><span class="p">},</span> <span class="n">axis</span><span class="o">=</span><span class="s">'columns'</span><span class="p">)</span>
<span class="n">stat_sig_df</span>
</code></pre></div></div>

<div>
  <style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-aligned: middle;
    }

    .dataframe tbody tr th {
        vertical-aligned: top;
    }

    .dataframe thead th {
        text-aligned: right;
    }
</style>

  <table border="1" class="dataframe">
  <thead>
    <tr style="text-aligned: right;">
      <th></th>
      <th>model_pval</th>
      <th>log_model_pval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>cylinders</th>
      <td>0.158026</td>
      <td>0.275162</td>
    </tr>
    <tr>
      <th>horsepower</th>
      <td>0.321204</td>
      <td>0.000050</td>
    </tr>
    <tr>
      <th>acceleration</th>
      <td>0.307759</td>
      <td>0.005869</td>
    </tr>
  </tbody>
</table>
</div>

<p>So the original and log models are in total agreement about which features are significant!</p>

<p>Let’s look at prediction accuracy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="c"># split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">auto</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'mpg'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">(),</span> <span class="n">auto</span><span class="o">.</span><span class="n">mpg</span><span class="p">)</span>

<span class="c"># transform</span>
<span class="n">log_X_train</span><span class="p">,</span> <span class="n">log_X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c"># train models</span>
<span class="n">reg_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">log_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">log_X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># get train mean squared errors</span>
<span class="n">reg_train_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="n">log_train_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">log_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">log_X_train</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"The reg model train mse is {} and the log model train mse is {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">reg_train_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">log_train_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>

<span class="c"># get test mean squared errors</span>
<span class="n">reg_test_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="n">log_test_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">log_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">log_X_test</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"The reg model test mse is {} and the log model test mse is {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">reg_test_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">log_test_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The reg model train mse is 11.111 and the log model train mse is 9.496
The reg model test mse is 10.434 and the log model test mse is 8.83
</code></pre></div></div>

<p>From a prediction standpoint, the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">log(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span> model is an improvement</p>

<h3 id="the-square-root-model">The square root model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sqrt_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.834</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.831</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   279.5</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 28 Oct 2018</td> <th>  Prob (F-statistic):</th> <td>1.76e-147</td>
</tr>
<tr>
  <th>Time:</th>                 <td>19:29:40</td>     <th>  Log-Likelihood:    </th> <td> -1023.0</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>   397</td>      <th>  AIC:               </th> <td>   2062.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   389</td>      <th>  BIC:               </th> <td>   2094.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P&gt;|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>        <td>  -51.9765</td> <td>    9.138</td> <td>   -5.688</td> <td> 0.000</td> <td>  -69.942</td> <td>  -34.011</td>
</tr>
<tr>
  <th>cylinders</th>    <td>   -0.0144</td> <td>    1.535</td> <td>   -0.009</td> <td> 0.993</td> <td>   -3.031</td> <td>    3.003</td>
</tr>
<tr>
  <th>displacement</th> <td>    0.2176</td> <td>    0.229</td> <td>    0.948</td> <td> 0.344</td> <td>   -0.234</td> <td>    0.669</td>
</tr>
<tr>
  <th>horsepower</th>   <td>   -0.6775</td> <td>    0.303</td> <td>   -2.233</td> <td> 0.026</td> <td>   -1.274</td> <td>   -0.081</td>
</tr>
<tr>
  <th>weight</th>       <td>   -0.6471</td> <td>    0.078</td> <td>   -8.323</td> <td> 0.000</td> <td>   -0.800</td> <td>   -0.494</td>
</tr>
<tr>
  <th>acceleration</th> <td>   -0.5983</td> <td>    0.821</td> <td>   -0.729</td> <td> 0.467</td> <td>   -2.212</td> <td>    1.016</td>
</tr>
<tr>
  <th>year</th>         <td>   12.9347</td> <td>    0.854</td> <td>   15.139</td> <td> 0.000</td> <td>   11.255</td> <td>   14.614</td>
</tr>
<tr>
  <th>origin</th>       <td>    3.2448</td> <td>    0.763</td> <td>    4.253</td> <td> 0.000</td> <td>    1.745</td> <td>    4.745</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>38.601</td> <th>  Durbin-Watson:     </th> <td>   1.306</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  69.511</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.589</td> <th>  Prob(JB):          </th> <td>8.05e-16</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.677</td> <th>  Cond. No.          </th> <td>3.30e+03</td>
</tr>
</table>
<p><br /><br />Warnings:<br />[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br />[2] The condition number is large, 3.3e+03. This might indicate that there are<br />strong multicollinearity or other numerical problems.</p>

<p>The <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> value is slightly less than for the log model, but not much, and the F-statistic p-value is comparable. 
This model doesn’t like <code class="highlighter-rouge">cylinder</code> and <code class="highlighter-rouge">displacement</code> just like the regular and log models, but also rejects <code class="highlighter-rouge">acceleration</code>.</p>

<p>Now we’ll check prediction accuracy</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># transform</span>
<span class="n">sqrt_X_train</span><span class="p">,</span> <span class="n">sqrt_X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c"># train sqrt model</span>
<span class="n">sqrt_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sqrt_X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># get train mean squared errors</span>
<span class="n">reg_train_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="n">sqrt_train_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">sqrt_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sqrt_X_train</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"The reg model train mse is {} and the sqrt model train mse is {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">reg_train_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">sqrt_train_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>

<span class="c"># get test mean squared errors</span>
<span class="n">reg_test_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="n">sqrt_test_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">sqrt_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sqrt_X_test</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"The reg model test mse is {} and the sqrt model test mse is {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">reg_test_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">sqrt_test_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The reg model train mse is 11.111 and the sqrt model train mse is 10.365
The reg model test mse is 10.434 and the sqrt model test mse is 9.635
</code></pre></div></div>

<p>Again, the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><mi>X</mi></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.11333499999999996em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9266650000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span><span style="top:-2.886665em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5, -10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8, -50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0, 35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5, -221c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467 s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422 s-65,47,-65,47z M834 80H400000v40H845z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.11333499999999996em;"><span></span></span></span></span></span></span></span></span> model is better at prediction</p>

<h3 id="the-squared-model">The squared model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sq_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.798</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.794</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   219.3</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 28 Oct 2018</td> <th>  Prob (F-statistic):</th> <td>8.35e-131</td>
</tr>
<tr>
  <th>Time:</th>                 <td>19:38:32</td>     <th>  Log-Likelihood:    </th> <td> -1062.3</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>   397</td>      <th>  AIC:               </th> <td>   2141.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   389</td>      <th>  BIC:               </th> <td>   2172.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P&gt;|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th>        <td>    0.9215</td> <td>    2.352</td> <td>    0.392</td> <td> 0.695</td> <td>   -3.702</td> <td>    5.546</td>
</tr>
<tr>
  <th>cylinders</th>    <td>   -0.0864</td> <td>    0.025</td> <td>   -3.431</td> <td> 0.001</td> <td>   -0.136</td> <td>   -0.037</td>
</tr>
<tr>
  <th>displacement</th> <td> 5.672e-05</td> <td> 1.39e-05</td> <td>    4.092</td> <td> 0.000</td> <td> 2.95e-05</td> <td>  8.4e-05</td>
</tr>
<tr>
  <th>horsepower</th>   <td>-2.945e-05</td> <td> 4.98e-05</td> <td>   -0.591</td> <td> 0.555</td> <td>   -0.000</td> <td> 6.85e-05</td>
</tr>
<tr>
  <th>weight</th>       <td>-9.535e-07</td> <td> 8.95e-08</td> <td>  -10.653</td> <td> 0.000</td> <td>-1.13e-06</td> <td>-7.77e-07</td>
</tr>
<tr>
  <th>acceleration</th> <td>    0.0066</td> <td>    0.003</td> <td>    2.466</td> <td> 0.014</td> <td>    0.001</td> <td>    0.012</td>
</tr>
<tr>
  <th>year</th>         <td>    0.0050</td> <td>    0.000</td> <td>   14.360</td> <td> 0.000</td> <td>    0.004</td> <td>    0.006</td>
</tr>
<tr>
  <th>origin</th>       <td>    0.4110</td> <td>    0.069</td> <td>    5.956</td> <td> 0.000</td> <td>    0.275</td> <td>    0.547</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>20.163</td> <th>  Durbin-Watson:     </th> <td>   1.296</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  27.033</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.421</td> <th>  Prob(JB):          </th> <td>1.35e-06</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.961</td> <th>  Cond. No.          </th> <td>1.45e+08</td>
</tr>
</table>
<p><br /><br />Warnings:<br />[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br />[2] The condition number is large, 1.45e+08. This might indicate that there are<br />strong multicollinearity or other numerical problems.</p>

<p>Slightly lower <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> and higher <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span></span></span></span>-statistic <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>-value than previous, but seems negligible (in all cases the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span></span></span></span>-statistic <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>-value is effectively zero)</p>

<p>Let’s check prediction accuracy</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># transform</span>
<span class="n">X_sq_train</span><span class="p">,</span> <span class="n">X_sq_test</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">X_test</span><span class="o">**</span><span class="mi">2</span>

<span class="c"># train sqrt model</span>
<span class="n">sq_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_sq_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># get train mean squared errors</span>
<span class="n">reg_train_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
<span class="n">sq_train_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">sq_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_sq_train</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"The reg model train mse is {} and the sq model train mse is {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">reg_train_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">sq_train_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>

<span class="c"># get test mean squared errors</span>
<span class="n">reg_test_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="n">sq_test_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">sq_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sq_X_test</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">"The reg model test mse is {} and the sq model test mse is {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">reg_test_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">sq_test_mse</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The reg model train mse is 11.111 and the sq model train mse is 12.571
The reg model test mse is 10.434 and the sq model test mse is 12.0
</code></pre></div></div>

<p>So the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>X</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">X^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> model is not as good at predicting as any of the other models</p>


</div>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', functione. {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  </body>
</html>
