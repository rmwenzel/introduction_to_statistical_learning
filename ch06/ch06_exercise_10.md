---
layout: page
title: 6. Linear Model Selection and Regularization
---

{% katexmm %}

# Exercise 10: Exploring test error on a simulated dataset

<div class="toc"><ul class="toc-item"><li><span><a href="#a-aenerate-the-data" data-toc-modified-id="a.-Generate-the-data-1">a. Generate the data</a></span></li><li><span><a href="#b-train-test-split" data-toc-modified-id="b.-Train-test-split-2">b. Train test split</a></span></li><li><span><a href="#c-bss-on-training-data-and-train-error" data-toc-modified-id="c.-BSS-on-training-data-and-train-error-3">c. BSS on training data and train error</a></span></li><li><span><a href="#d-bss-test-error" data-toc-modified-id="d-bss-test-error-4">d. BSS test error</a></span></li><li><span><a href="#e-model-with-minimum-test-error" data-toc-modified-id="e.-Model-with-minimum-test-error-5">e. Model with minimum test error</a></span></li><li><span><a href="#f-comparing-best-model-and-true-model" data-toc-modified-id="f.-Comparing-best-model-and-true-model-6">f. Comparing best model and true model</a></span></li></ul></div>

Note that this exercise has been modified to $p=15$, to be able to complete c. in a reasonable time (BSS algorithm runtime is exponential in the number of predictors). The training error as a function of $p$ is nearly constant when $p = 15$ anyway.

## a. Generate the data


```python
import numpy as np
import pandas as pd

# random X, coefficients, and noise
X = 1.1*np.random.rand(1000)
beta, e = np.random.rand(15, 1).flatten(), np.random.normal(size=1000)

# randomly zero 4 entries of beta
beta_zeros_indices = np.random.choice(15, 4)
beta = np.array([beta[i] if i not in beta_zeros_indices else 0 for i in range(len(beta))])

# data generated by degree 15 polynomial model
data = pd.DataFrame({'X^' + stri.: X**i for i in range(1, 16)})

# add response
data['y'] = np.matmul(data.values, beta) + e

data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X^1</th>
      <th>X^2</th>
      <th>X^3</th>
      <th>X^4</th>
      <th>X^5</th>
      <th>X^6</th>
      <th>X^7</th>
      <th>X^8</th>
      <th>X^9</th>
      <th>X^10</th>
      <th>X^11</th>
      <th>X^12</th>
      <th>X^13</th>
      <th>X^14</th>
      <th>X^15</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.571933</td>
      <td>0.327107</td>
      <td>0.187083</td>
      <td>0.106999</td>
      <td>0.061196</td>
      <td>0.035000</td>
      <td>0.020018</td>
      <td>0.011449</td>
      <td>6.547953e-03</td>
      <td>3.744989e-03</td>
      <td>2.141882e-03</td>
      <td>1.225013e-03</td>
      <td>7.006252e-04</td>
      <td>4.007106e-04</td>
      <td>2.291796e-04</td>
      <td>1.508114</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.179980</td>
      <td>0.032393</td>
      <td>0.005830</td>
      <td>0.001049</td>
      <td>0.000189</td>
      <td>0.000034</td>
      <td>0.000006</td>
      <td>0.000001</td>
      <td>1.981650e-07</td>
      <td>3.566581e-08</td>
      <td>6.419147e-09</td>
      <td>1.155321e-09</td>
      <td>2.079351e-10</td>
      <td>3.742424e-11</td>
      <td>6.735629e-12</td>
      <td>-0.155696</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.095461</td>
      <td>1.200035</td>
      <td>1.314592</td>
      <td>1.440084</td>
      <td>1.577557</td>
      <td>1.728152</td>
      <td>1.893123</td>
      <td>2.073843</td>
      <td>2.271815e+00</td>
      <td>2.488685e+00</td>
      <td>2.726257e+00</td>
      <td>2.986509e+00</td>
      <td>3.271605e+00</td>
      <td>3.583916e+00</td>
      <td>3.926041e+00</td>
      <td>11.183512</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.644125</td>
      <td>0.414897</td>
      <td>0.267245</td>
      <td>0.172139</td>
      <td>0.110879</td>
      <td>0.071420</td>
      <td>0.046003</td>
      <td>0.029632</td>
      <td>1.908667e-02</td>
      <td>1.229420e-02</td>
      <td>7.918998e-03</td>
      <td>5.100823e-03</td>
      <td>3.285566e-03</td>
      <td>2.116315e-03</td>
      <td>1.363171e-03</td>
      <td>0.783877</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.003829</td>
      <td>1.007672</td>
      <td>1.011530</td>
      <td>1.015403</td>
      <td>1.019291</td>
      <td>1.023193</td>
      <td>1.027111</td>
      <td>1.031043</td>
      <td>1.034991e+00</td>
      <td>1.038954e+00</td>
      <td>1.042932e+00</td>
      <td>1.046925e+00</td>
      <td>1.050933e+00</td>
      <td>1.054957e+00</td>
      <td>1.058996e+00</td>
      <td>4.482393</td>
    </tr>
  </tbody>
</table>
</div>



## b. Train test split


```python
from sklearn.model_selection import train_test_split
```


```python
%%capture

X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['y']), 
                                                    data['y'], train_size=100)
```


```python
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```




    ((100, 15), (900, 15), (100,), (900,))



## c. BSS on training data and train error


```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

# dict for mse results
bss_mses = {'num_pred': [], 'best_pred_idx': [], 'best_mse_train': []}

for k in range(1, 16):
    reg = LinearRegression()
    efs = EFS(reg, min_features=k, max_features=k, scoring='neg_mean_squared_error',
              print_progress=False, cv=None, n_jobs=-1)
    efs = efs.fit(X_train, y_train)
    bss_mses['num_pred'] += [k]
    bss_mses['best_pred_idx'] += [efs.best_idx_]
    bss_mses['best_mse_train'] += [-efs.best_score_]
```


```python
bss_mses_df = pd.DataFrame(bss_mses)
bss_mses_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num_pred</th>
      <th>best_pred_idx</th>
      <th>best_mse_train</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>(8,)</td>
      <td>0.965216</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>(7, 14)</td>
      <td>0.960280</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>(3, 4, 14)</td>
      <td>0.955215</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>(5, 6, 7, 8)</td>
      <td>0.950193</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>(2, 4, 5, 6, 7)</td>
      <td>0.932734</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>(0, 1, 2, 3, 4, 6)</td>
      <td>0.868717</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>(0, 2, 3, 4, 5, 6, 11)</td>
      <td>0.856748</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>(0, 2, 3, 4, 5, 6, 8, 14)</td>
      <td>0.856729</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>(0, 1, 5, 6, 7, 9, 10, 11, 12)</td>
      <td>0.856044</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>(0, 1, 6, 7, 9, 10, 11, 12, 13, 14)</td>
      <td>0.855568</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>(0, 1, 2, 7, 8, 9, 10, 11, 12, 13, 14)</td>
      <td>0.855489</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)</td>
      <td>0.854482</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)</td>
      <td>0.849140</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)</td>
      <td>0.848783</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>
      <td>0.846343</td>
    </tr>
  </tbody>
</table>
</div>




```python
import seaborn as sns

sns.lineplot(x=bss_mses_df['num_pred'], y=bss_mses_df['best_mse_train'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a19c8cc88>




![png]({{site.baseurl}}/assets/images/ch06_exercise_10_11_1.png)


## d. BSS test error


```python
# helper function which creates a full length beta with zero entries for ommitted predictors
def full_beta(beta_len, model_beta, pred_idx):
    beta, counter = np.zeros(beta_len), 0
    for i in pred_idx:
        beta[i] = model_beta[counter]
        counter += 1
    return beta

# helper which predicts test data >= features of train data
def diff_num_feat_pred(estimator, X_train, y_train, X_test, pred_idx):
    if len(pred_idx) == 1:
        model_beta = estimator().fit(X_train[:, pred_idx].reshape(-1, 1), y_train).coef_
    else:
        model_beta = estimator().fit(X_train[:, pred_idx], y_train).coef_
    beta_len = X_test.shape[1]
    beta = full_beta(beta_len, model_beta, pred_idx)
    return np.matmul(X_test, beta)

```


```python
from sklearn.metrics import mean_squared_error

# track best model test error
bss_mses_df['best_mse_test'] = np.zeros(len(bss_mses_df))

for k in bss_mses_df.index:
    pred_idx = bss_mses_df.loc[k, 'best_pred_idx']
    y_pred = diff_num_feat_pred(LinearRegression, X_train.values, y_train, X_test, pred_idx)
    bss_mses_df.loc[k, 'best_mse_test'] = mean_squared_error(y_test, y_pred)
```


```python
bss_mses_df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num_pred</th>
      <th>best_pred_idx</th>
      <th>best_mse_train</th>
      <th>best_mse_test</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>(8,)</td>
      <td>0.965216</td>
      <td>1.330568</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>(7, 14)</td>
      <td>0.960280</td>
      <td>1.269383</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>(3, 4, 14)</td>
      <td>0.955215</td>
      <td>1.316338</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>(5, 6, 7, 8)</td>
      <td>0.950193</td>
      <td>1.337739</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>(2, 4, 5, 6, 7)</td>
      <td>0.932734</td>
      <td>1.159891</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>(0, 1, 2, 3, 4, 6)</td>
      <td>0.868717</td>
      <td>7.432659</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>(0, 2, 3, 4, 5, 6, 11)</td>
      <td>0.856748</td>
      <td>4.303755</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>(0, 2, 3, 4, 5, 6, 8, 14)</td>
      <td>0.856729</td>
      <td>4.327183</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>(0, 1, 5, 6, 7, 9, 10, 11, 12)</td>
      <td>0.856044</td>
      <td>4.543950</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>(0, 1, 6, 7, 9, 10, 11, 12, 13, 14)</td>
      <td>0.855568</td>
      <td>4.293005</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>(0, 1, 2, 7, 8, 9, 10, 11, 12, 13, 14)</td>
      <td>0.855489</td>
      <td>4.556714</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)</td>
      <td>0.854482</td>
      <td>3.741683</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)</td>
      <td>0.849140</td>
      <td>2.879799</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14)</td>
      <td>0.848783</td>
      <td>2.322990</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>
      <td>0.846343</td>
      <td>31.446850</td>
    </tr>
  </tbody>
</table>
</div>




```python
sns.lineplot(x=bss_mses_df['num_pred'], y=bss_mses_df['best_mse_test'])
```




    <matplotlib.axes._subplots.AxesSubplot at 0x1a19c18160>




![png]({{site.baseurl}}/assets/images/ch06_exercise_10_16_1.png)


## e. Model with minimum test error

Here is the best model by test mse:


```python
bss_mses_df.loc[bss_mses_df['best_mse_test'].idxmin(), :]
```




    num_pred                        5
    best_pred_idx     (2, 4, 5, 6, 7)
    best_mse_train           0.932734
    best_mse_test             1.15989
    Name: 4, dtype: object



And by train mse:


```python
bss_mses_df.loc[bss_mses_df['best_mse_train'].idxmin(), :]
```




    num_pred                                                         15
    best_pred_idx     (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...
    best_mse_train                                             0.846343
    best_mse_test                                               31.4469
    Name: 14, dtype: object



The best model by test mse is not the best model by train mse

## f. Comparing best model and true model

The best model by test mse has only one third of the predictors of the full model. This is perhaps not surprising if we inspect the distributions of the features


```python
from itertools import product
import matplotlib.pyplot as plt

sns.set()
f, axes = plt.subplots(5, 3, figsize=(15, 15), sharex=True)
prod = list(product(range(5), range(3)))
for (i, j) in prod:
    sns.distplot(data.iloc[:, prod.index((i,j))], ax=axes[i, j])
```


![png]({{site.baseurl}}/assets/images/ch06_exercise_10_25_0.png)


The higher the power of $X$, the more concentrated the values are around 9

{% endkatexmm %}
