
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{ch08\_notes}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    Table of Contents{}

{{8~~}Tree-Based Methods}

{{8.1~~}The Basics of Decision Trees}

{{8.1.1~~}Regression Trees}

{{8.1.2~~}Classification Trees}

{{8.1.3~~}Trees Versus Linear Models}

{{8.1.4~~}Advantages and Disadvantages of Trees}

{{8.2~~}Bagging, Random Forests, Boosting}

{{8.2.1~~}Bagging}

{{8.2.2~~}Random Forests}

{{8.2.3~~}Boosting}

{{8.3~~}Footnotes}

{{8.3.1~~}blah}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{tree-based-methods}{%
\section{Tree-Based Methods}\label{tree-based-methods}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{the-basics-of-decision-trees}{%
\subsection{The Basics of Decision
Trees}\label{the-basics-of-decision-trees}}

    \hypertarget{regression-trees}{%
\subsubsection{Regression Trees}\label{regression-trees}}

    \hypertarget{overview}{%
\subparagraph{Overview}\label{overview}}

    \begin{itemize}
\tightlist
\item
  There are two main steps:

  \begin{itemize}
  \tightlist
  \item
    Partition predictor space \(\mathbb{R}^p\) into regions
    \(R_1, \dots, R_M\).
  \item
    For all \(X = (X_1, \dots, X_p) \in R_m\), predict the average over
    the responses in \(R_m\)
    \[\hat{f}(X) = \hat{y}_{R_m} := \frac{1}{N_m}\sum_{i: y_i\in R_m} y_i\]
    where \(N_m = |\{y_i\ |\ y_i \in R_m\}|\)
  \end{itemize}
\item
  In practice, we take the regions of the partition to be rectangular
  for simplicity and ease of interpretation.
\item
  We choose the partition to minimize the RSS
  \[ \sum_{m = 1}^M \sum_{i: y_i \in R_m} (y_i - \hat{y}_{R_m})^2 \]
\item
  We search the space of partitions using a \textbf{\emph{recursive
  binary splitting}}64 strategy.
\end{itemize}

    \hypertarget{algorithm-recursive-binary-decision-tree-for-linear-regression}{%
\subparagraph{Algorithm: Recursive Binary Decision Tree for Linear
Regression}\label{algorithm-recursive-binary-decision-tree-for-linear-regression}}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start with top node \(\mathbb{R}^p\)
\item
  While a stopping criterion is unsatisfied:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Let
  \end{enumerate}

  \[(\hat{i}, \hat{j}) = \underset{(i, j)}{\text{argmin}}\left(
                                \sum_{i: x_i\in R_1} (y_i - \hat{y}_{R_1})^ 2 + 
                                \sum_{i: x_i\in R_2} (y_i - \hat{y}_{R_2})^ 2\right)\]
  where

  \[R_{1} = \{X| X_j < x_{i,j}\}\]
  \[R_{2} = \{X| X_j \geqslant x_{i,j}\}\]

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{1}
  \item
    Add nodes

    \[\hat{R}_{1} = \{X| X_\hat{j} < x_{\hat{i},\hat{j}}\}\]
    \[\hat{R}_{2} = \{X| X_\hat{j} \geqslant x_{\hat{i},\hat{j}}\}\]

    to the partition, and recurse on one of the nodes
  \end{enumerate}
\end{enumerate}

    \hypertarget{tree-pruning}{%
\subparagraph{Tree-pruning}\label{tree-pruning}}

    \begin{itemize}
\tightlist
\item
  Complex trees can overfit, but simpler trees may avoid it 65.
\item
  To get a simpler tree, we can grow a large tree \(T_0\) and
  \textbf{\emph{prune}} it to obtain a subtree.
\item
  \textbf{\emph{Cost complexity}} or \textbf{\emph{weakest link}}
  pruning is a method for finding an optimal subtree 66. For
  \(\alpha > 0\), we obtain a subtree
  \[ T_\alpha = \underset{T\ \subset T_0}{\text{argmin}}
                  \left(\sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} \left(y_i - \hat{y}_{R_m}\right)^2 + \alpha|T|\right)\]
  where \(|T|\) is the number of terminal nodes of \(T\), \(R_m\) is the
  rectangle corresponding to the \(m\)-th terminal node, and
  \(\hat{y}_{R_m}\) is the predicted response (average of
  \(y_i\in R_m\)) 67.
\end{itemize}

    \hypertarget{algorithm-weakest-link-regression-tree-with-k-fold-cross-validation}{%
\subparagraph{\texorpdfstring{Algorithm: Weakest link regression tree
with \(K\)-fold
cross-validation}{Algorithm: Weakest link regression tree with K-fold cross-validation}}\label{algorithm-weakest-link-regression-tree-with-k-fold-cross-validation}}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For each68 \(\alpha > 0\):

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    For \(k = 1, \dots K\):

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      Let
      \(\mathcal{D}_k = \mathcal{D} \backslash \{k-\text{th fold}\}\)
    \item
      Use recursive binary splitting to grow a tree \(T_{k}\), stopping
      when each node has fewer than some minimum number of observations
      \(M\) is reached 69
    \item
      Use weakest link pruning to find a subtree \(T_{k, \alpha}\)
    \end{enumerate}
  \item
    Let \(CV_{(k)}(\alpha)\) be the \(K\)-fold cross-validation estimate
    of the mean squared test error
  \end{enumerate}
\item
  Choose
  \[ \hat{\alpha} = \underset{\alpha}{\text{argmin}}\ CV_{(k)}(\alpha) \]
\item
  Return \(\hat{T} = T_{\hat{\alpha}}\)
\end{enumerate}

    \hypertarget{classification-trees}{%
\subsubsection{Classification Trees}\label{classification-trees}}

    \begin{itemize}
\tightlist
\item
  \textbf{\emph{Classification trees}} are very similar to regression
  trees, but they predict qualitative responses. The predicted class for
  an observation \((x_i, y_i)\) in \(R_m\) is 70 is
  \[ \hat{k}_m = \underset{k}{\text{argmax}}\ \hat{p}_{m,k} \] where
  \(\hat{p}_{m,k}\) is the fraction of observations \((x_i, y_i)\) in
  the region \(R_m\) such that \(y_i = k\).
\item
  One performance measure is the \textbf{\emph{Classification error
  rate}}71 for the region \(R_m\) is
  \[ E_m = 1 - \hat{p}_{m, \hat{k}} \]
\item
  A better performance measure is the \textbf{\emph{Gini index}} for the
  region \(R_m\), a measure of total variance 72 across classes
  \[ G_m = \sum_{k = 1}^K \hat{p}_{m,k}(1 - \hat{p}_{m,k})\]
\item
  Another better performance measure is the \textbf{\emph{entropy}} for
  the region \(R_m\)73
  \[ D_m = \sum_{k = 1}^K - \hat{p}_{m,k}\log(\hat{p}_{m,k}) \]
\item
  Typically the Gini index or entropy is used to prune, due to their
  sensitivity to node purity. However, if prediction accuracy is the
  goal then classification error rate is preferable.
\end{itemize}

    \hypertarget{trees-versus-linear-models}{%
\subsubsection{Trees Versus Linear
Models}\label{trees-versus-linear-models}}

    \begin{itemize}
\item
  A linear regression model is of the form

  \[f(X) = \beta_0 + \sum_{j = 1}^p \beta_j X_j\]

  while a regression tree model is of the form

  \[ f(X) = \sum_{m = 1}^M c_m I(X \in R_m)\]
\item
  Linear regression will tend to perform better if the relationship
  between features and response is well-approximated by a linear
  function, whereas the regression tree will tend perform better if the
  relationship is non-linear or complex.
\end{itemize}

    \hypertarget{advantages-and-disadvantages-of-trees}{%
\subsubsection{Advantages and Disadvantages of
Trees}\label{advantages-and-disadvantages-of-trees}}

    \hypertarget{advantages}{%
\subparagraph{Advantages}\label{advantages}}

    \begin{itemize}
\tightlist
\item
  Conceptual simplicity
\item
  May mirror human decision-making better than previous regression and
  classification methods
\item
  Readily visualizable and easily interpreted
\item
  Can handle qualitative predictors without the need for dummy variables
\end{itemize}

    \hypertarget{disadvantages}{%
\subparagraph{Disadvantages}\label{disadvantages}}

    \begin{itemize}
\tightlist
\item
  Less accurate prediction than previous regression and classification
  methods
\item
  Non-robust to changes in data -- small changes in data lead to large
  changes in estimated tree.
\end{itemize}

    \hypertarget{bagging-random-forests-boosting}{%
\subsection{Bagging, Random Forests,
Boosting}\label{bagging-random-forests-boosting}}

    These are methods for improving the prediction accuracy of decision
trees.

    \hypertarget{bagging}{%
\subsubsection{Bagging}\label{bagging}}

    \begin{itemize}
\item
  The decision trees in Section \ref{the-basics-of-decision-trees}
  suffer from high variance.
\item
  \textbf{\emph{Bagging}} is a method of reducing the variance of a
  statistical learning process 74. The bagging estimate of the target
  function of the process with dataset \(\mathcal{D}\) is

  \[\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b = 1}^B \hat{f}^{*b}(x) \]

  where \(\hat{f^*}^b(x)\) is the estimate of target function on the
  boostrap dataset \(\mathcal{D}_b\) 75.
\item
  Bagging can be used for any statistical learning method but it is
  particularly useful for decision trees.76
\end{itemize}

    \hypertarget{out-of-bag-error-estimation}{%
\subparagraph{Out-of-bag Error
Estimation}\label{out-of-bag-error-estimation}}

    \begin{itemize}
\tightlist
\item
  On average, a bagged tree uses about 2/3 of the data -- the remaining
  1/3 is the \textbf{\emph{out-of-bag}} (OOB) data.
\item
  We can predict the response for each observation using the trees for
  which it was OOB, yielding about B/3 prediction.
\item
  If we're doing regression, we can average these predicted responses,
  or if we're doing classification, take a majority vote, to get a
  single OOB prediction for each observation.
\item
  Test error can be estimated using these predictions.
\end{itemize}

    \hypertarget{variable-importance-measures}{%
\subparagraph{Variable Importance
Measures}\label{variable-importance-measures}}

    \begin{itemize}
\tightlist
\item
  Bagging typically results in improved prediction accuracy over single
  trees, at the expense of interpretability
\item
  The RSS (for bagging regression trees) and Gini index (for bagging
  classification trees) can provide measures of variable importance.
\item
  For both loss functions (RSS/Gini) the amount the loss is decreases
  due to a split over a given predictor, averaged over the B bagged
  trees. The greater the decrease, the more important the predictor
\end{itemize}

    \hypertarget{random-forests}{%
\subsubsection{Random Forests}\label{random-forests}}

    \begin{itemize}
\tightlist
\item
  Random forests works as follows: at each split in the tree, choose a
  predictor from among a new random sample of
  \(1 \leqslant m \leqslant p\) predictors.
\item
  The random predictor sampling overcomes the tendency of bagged trees
  to look similar given strong predictors (e.g.~the strongest predictor
  will be at the top of most or all of the bagged trees).
\item
  On average, \(\frac{p-m}{p}\) of the splits will not consider a given
  predictor, giving other predictors a chance to be chosen. This
  decorrelation of the trees improves the reduction in variance achieved
  by bagged.
\item
  \(m=p\) corresponds to bagging. \(m << p\) is useful when there is a
  large number of correlated predictors. Typically we choose
  \(m \approx \sqrt{p}\)
\end{itemize}

    \hypertarget{boosting}{%
\subsubsection{Boosting}\label{boosting}}

    \begin{itemize}
\tightlist
\item
  Boosting is another method of improving prediction accuracy that can
  be applied to many statistical learning methods.
\item
  In decision trees, each tree is build using information from the
  previous trees. Instead of bootstrapped datasets, the datasets are
  modified based on the previously grown trees.
\item
  The boosting approach learns slowly, by slowly improving in areas
  where it underperforms. It has 3 parameters:

  \begin{itemize}
  \tightlist
  \item
    Number of trees \(B\). Unlike bagging and random forests, boosting
    can overfit if \(B\) is too big, although this happens slowly.
  \item
    The shrinkage parameter \(\lambda > 0\). Typical values are
    \(\lambda = 0.01, 0.001\). Very small \(\lambda\) can require very
    large \(B\) to get good performance.
  \item
    The number of tree splits \(d\), which controls the complexity of
    the boosted ensemble. Often \(d\) works well (the resulting tree is
    called a \textbf{\emph{stump}}). 77
  \end{itemize}
\end{itemize}

    \hypertarget{algorithm-boosting-for-regression-trees}{%
\subparagraph{Algorithm: Boosting for Regression
Trees}\label{algorithm-boosting-for-regression-trees}}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Set \(\hat{f}(x) = 0\) and \(r_i = y_i\),
  \(1 \leqslant i \leqslant n\)
\item
  For \(b = 1, 2, \dots, B\):

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Fit a tree \(\hat{f}^b\) with \(d\) splits to \((X, r)\)
  \item
    Update the model \(\hat{f}\) by adding a shrunk version of the new
    tree: \[ \hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)\]
  \item
    Update the residuals: \[ r_i \leftarrow r_i - \lambda \hat{f}^b(x)\]
  \end{enumerate}
\item
  Output the boosted model

  \[ \hat{f}(x) = \sum_{b = 1}^B \lambda \hat{f}^b(x)\]
\end{enumerate}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{footnotes}{%
\subsection{Footnotes}\label{footnotes}}

    \hypertarget{foot64}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{63}
\tightlist
\item
  This strategy results in a binary tree with the partition regions as
  leaves, and binary splits as nodes. It is ``top-down'' because it
  starts at the top of the partition tree (with a single region),
  ``binary'' because it splits the predictor space into two regions at
  each node in the tree, ``recursive'' because it calls itself at each
  node, and ``greedy'' because at each node, it chooses the optimal
  split at that node. ↩
\end{enumerate}

\hypertarget{foot65}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{64}
\tightlist
\item
  That is, it may lead to lower variance and better interpretation at
  the cost of a higher bias ↩
\end{enumerate}

\hypertarget{foot66}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{65}
\tightlist
\item
  We want a subtree with minimal estimated test error but it's
  infeasible to compute this for all subtrees. ↩
\end{enumerate}

\hypertarget{foot67}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{66}
\tightlist
\item
  This is the RSS for the partition given by the nodes of the tree
  \(T\), with a weighted penalty \(\alpha|T|\) for the number of nodes
  (hence the complexity). ↩
\end{enumerate}

\hypertarget{foot68}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{67}
\tightlist
\item
  Even though \(\alpha \in [0, \infty)\) is a continuous parameter here,
  in practice it will be selected from a finite set of values. In fact
  (cf.~comment on pg 309 of the text), as \(\alpha\)
  increases,``branches get pruned in a nested and predictable fashion'',
  resulting in a sequence of subtrees as a function of \(\alpha\). One
  can then find a sequence \(\alpha_1, \dots, \alpha_N\) such that at
  each \(\alpha_i\), a branch is removed, and since the tree is finite,
  the algorithm is guaranteed to terminate. ↩
\end{enumerate}

\hypertarget{foot69}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{68}
\tightlist
\item
  The smallest possible number of observations per node is \(M=1\),
  which results in a partition with only one point in each region. This
  is clearly a maximal complexity tree, so we probably take \(M >> 1\)
  in practice. ↩
\end{enumerate}

\hypertarget{foot70}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{69}
\tightlist
\item
  That is, the predicted class for observations in \(R_m\) is the most
  frequently occuring class in \(R_m\). ↩
\end{enumerate}

\hypertarget{foot71}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{70}
\tightlist
\item
  The classification error rate isn't sufficiently sensitive to ``node
  purity'', that is degree to which a node contains observations from a
  single class. ↩
\end{enumerate}

\hypertarget{foot72}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{71}
\tightlist
\item
  The Gini index is a measure of ``node purity'' -- it is minimized when
  all \(\hat{p}_{m, k} \in \{0, 1\}\), that is, when all nodes contain
  observations from a single class. ↩
\end{enumerate}

\hypertarget{foot73}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{72}
\tightlist
\item
  The \(\hat{p}_{m,k}\) are the empirical pmf estimates of the
  conditional probabilities \(p_{m, k} = P(Y = k | X \in R_m)\), so
  \(D\) is an estimate of the conditional entropy, i.e.~the entropy of
  \(Y\ |\ X \in R_m\). Thus \(D\) is a measure of information that the
  empirical pmf, and hence the corresponding tree provides, that is, of
  its average suprisal.
\end{enumerate}

As with the Gini index, \(D\) is minimized when all
\(\hat{p}_{m, k} \in \{0, 1\}\). An average surprisal of zero means the
tree provides all information, that is, it perfectly separates the
classes. ↩

\hypertarget{foot74}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{73}
\tightlist
\item
  Bagging is another name for bootstrapping. It appears that the latter
  is usually used in the context of estimating the standard error of a
  statistic, while the former is used in the context of a statistical
  learning process (even though these are essentially the same). ↩
\end{enumerate}

\hypertarget{foot75}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{74}
\tightlist
\item
  Really this is the bootstrap estimate of the average of the target
  function estimate over many datasets. For a given dataset
  \(\mathcal{D}\), the function \(\hat{f}(x)\) produced by the learning
  process is an estimate of the target function \(f(x)\). Repeating the
  process \(1 \leqslant b \leqslant B\) times over datasets
  \(\mathcal{D}_b\), we get estimates \(\hat{f}^b(x)\). Assuming these
  are iid, they have common variance \(\sigma^2\), but their average
\end{enumerate}

\[\hat{f}_{\text{avg}} = \frac{1}{B} \sum_{b = 1}^B \hat{f}^{b}(x)\]

has variance \(\frac{\sigma^2}{B}\). Given \(B\) large enough, this
variance is low. Bagging/bootstrapping gets around the lack of separate
datasets \(\mathcal{D}_b\) in practice by repeated sampling with
replacement from a single dataset \(\mathcal{D}\). ↩

\hypertarget{foot76}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{75}
\tightlist
\item
  For regression, one grows \(B\) deep (unpruned) regression trees on
  \(B\) bootstrapped datasets, each of which has
  Section \ref{the-bias-variance-tradeoff}, then averages them to get a
  bootstrap estimate which has the same low bias, but much lower
  variance. For classification (since we can't average over the classes
  of the bootstrapped trees) a simple approach is to predict the
  majority class over the bootstrapped trees. ↩
\end{enumerate}

\hypertarget{foot77}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{76}
\tightlist
\item
  In the case of a stump, the boosted ensemble is fitting an additive
  model, since each term is a single variable. More generally, \(d\) is
  the interaction depth -- since \(d\) splits can involve at most \(d\)
  variables, this controls the interaction order of the boosted model
  (i.e.~the model can fit interaction terms up to degree \(d\)). ↩
\end{enumerate}

    \hypertarget{blah}{%
\subsubsection{blah}\label{blah}}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
