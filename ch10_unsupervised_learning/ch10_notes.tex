
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{ch10\_notes}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    Table of Contents{}

{{10~~}Unsupervised Learning}

{{10.1~~}The Challenge of Unsupervised Learning}

{{10.2~~}Principal Components Analysis}

{{10.2.1~~}What Are Principal Components?}

{{10.2.2~~}Another Interpretation of Principal Components}

{{10.2.3~~}More on PCA}

{{10.2.4~~}Other Uses for Principal Components}

{{10.3~~}Clustering Methods}

{{10.3.1~~}\(K\)-Means Clustering}

{{10.3.2~~}Hierarchical Clustering}

{{10.3.3~~}Practical Issues in Clustering}

{{10.4~~}Footnotes}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{unsupervised-learning}{%
\section{Unsupervised Learning}\label{unsupervised-learning}}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

    \hypertarget{the-challenge-of-unsupervised-learning}{%
\subsection{The Challenge of Unsupervised
Learning}\label{the-challenge-of-unsupervised-learning}}

    \begin{itemize}
\tightlist
\item
  \textbf{\emph{Unsupervised learning}} is learning in the absence of a
  response. It is often part of \textbf{\emph{exploratory data
  analysis}} (EDA).
\item
  Without a response, we aren't intested in prediction or
  classification, rather we are interested in discovering interesting
  things about the data. This can be difficult because such a goal is
  somewhat subjective.
\item
  Objective performance critera for unsupervised learning can also be
  challenging.
\end{itemize}

    \hypertarget{principal-components-analysis}{%
\subsection{Principal Components
Analysis}\label{principal-components-analysis}}

    \begin{itemize}
\tightlist
\item
  Principal components were discussed earlier as a dimensional reduction
  methof in the Section \ref{principal-components-regression}. They
  provide a low-dimensional representation of the data that contains as
  much variation as possible.
\item
  \textbf{\emph{Principal Components Analysis}} is the process of
  computing principal components and using them in data analysis.
\end{itemize}

    \hypertarget{what-are-principal-components}{%
\subsubsection{What Are Principal
Components?}\label{what-are-principal-components}}

    \begin{itemize}
\item
  The \textbf{\emph{first principal component}} of features
  \(X_1, \dots, X_p\) is the normalized linear combination
  \[ Z_1 = \hat{\phi}_1^\top X\] where
  \(X = (X_1, \dots, X_p), \hat{\phi}_1\in \mathbb{R}^p\) and
  \(|| \hat{\phi} || = 1\). The vector \(\hat{\phi}_1\) is called the
  \textbf{\emph{loading}} vector (its entries are called the
  \textbf{\emph{loadings}}) and
  \[ \hat{\phi}_1 = \underset{\underset{||\phi|| = 1}{\phi \in \mathbb{R}^p}}{\text{argmax}}\left(\frac{1}{n}\sum_{i=1}^n \left(\phi^\top x_i\right)^2\right)\]
\item
  Assume we have data \(X_i\) with features \(X_1, \dots, X_p\) which is
  centered in the features (each feature has mean zero). The objective
  function in the above optimization problem can be rewritten
  \[ \hat{\phi}_1 = \underset{\phi \in \mathbb{R}^p}{\text{argmax}}\left(\frac{1}{n}\sum_{i=1}^n ||z_i ||^2\right)\]
  which is just the sample variance. The \(z_{i1}\) are called the
  \textbf{\emph{scores}} of the first principal component \(Z_1\).
\item
  The first principal component has a nice geometric interpretation 85.
  The loading vector \(\phi_{1}\) defines a direction in
  \(\mathbb{R}^p\) along which the variation is maximized. The principal
  component scores \(z_{i1}\) are the projections of the data \(x_i\)
  onto \(\phi_1\) -- that is, the components of the \(x_i\) along this
  direction.
\item
  For \(j = 2,...,p\) we can compute the \(j\)-th principal component
  \(\phi_j\) recursively
\end{itemize}

\[ \hat{\phi}_j = \underset{\phi \in \mathbb{R}^p}{\text{argmax}}\left(\frac{1}{n}\sum_{i=1}^n \left(\phi^\top x_i\right)^2\right)\]

subject to 86

\[\phi_j^\top \phi_{j - 1} = 0\]. - We can plot the principal components
against each other for a low-dimensional visualization of the data. For
example a \textbf{\emph{biplot}} plots both the scores and the loading
vectors 87.

    \hypertarget{another-interpretation-of-principal-components}{%
\subsubsection{Another Interpretation of Principal
Components}\label{another-interpretation-of-principal-components}}

    \begin{itemize}
\tightlist
\item
  Principal components can also be seen as providing low-dimensional
  surfaces that are ``closest'' to the observations.
\item
  The span of the first \(M\) loading vectors \(\phi_1, \dots, \phi_M\)
  can be seen as the \(M\)-dimensional linear subspaces of
  \(\mathbb{R}^p\) which is closest to the observations \(x_i\) 88
\item
  Together the principal components \(Z_1, \dots, Z_M\) and loading
  vectors \(\phi_1, \dots, \phi_M\) can be seen as an \(M\)-dimensional
  approximation89 of each observation
\end{itemize}

\[x_{ij} \approx \sum_{m = 1}^M z_{im}\phi_{jm}\]

    \hypertarget{more-on-pca}{%
\subsubsection{More on PCA}\label{more-on-pca}}

    \begin{itemize}
\tightlist
\item
  PCA requires that the variables are centered to have mean zero
\item
  PCA is sensitive to scaling, so we usually scale each variable to have
  standard deviation 1.
\item
  Scaling to standard deviation 1 is particularly important when
  variables are measured in different units, however if they are
  measured in the same units we may not wish to do this.
\end{itemize}

    \hypertarget{uniqueness-of-the-principal-components}{%
\subparagraph{Uniqueness of the Principal
Components}\label{uniqueness-of-the-principal-components}}

    The loading vectors and score vectors are unique up to sign flips.

    \hypertarget{the-proportion-of-variance-explained}{%
\subparagraph{The Proportion of Variance
Explained}\label{the-proportion-of-variance-explained}}

    \begin{itemize}
\item
  How much of the information in a given data set is lost by projecting
  onto the principal components? More precisely, what is the
  \textbf{\emph{proportion of variance explained}} (PVE) by each
  principal component?
\item
  Assuming centered data, the \textbf{\emph{total variance}} 90 is
  \[\text{var}_{total} := \sum_{j = 1}^p \mathbb{V}(X_j) = \sum_{i = 1}^p \left(\frac{1}{n} \sum_{i = 1}^n x_{ij}^2 \right)\]

  while the \textbf{\emph{variance explained}} by the \(m\)-th principal
  component is
  \[\text{var}_{m} := \frac{1}{n} \sum_{i=1}^n z_{im}^2 = \frac{1}{n} \sum_{i = 1}^n \left(\sum_{i = 1}^p \phi_{jm}x_{ij} \right)^2 \].
\item
  The PVE of the \(m\)-th component is then

  \[\text{PVE}_m := \frac{\text{var}_{m}}{\text{var}_{total}}\]

  and the cumulative PVE of the first \(M\) components 91 is

  \[\sum_{m = 1}^M \text{PVE}_m \]
\end{itemize}

    \hypertarget{deciding-how-many-principal-components-to-use}{%
\subparagraph{Deciding How Many Principal Components to
Use}\label{deciding-how-many-principal-components-to-use}}

    \begin{itemize}
\tightlist
\item
  In general choose we may not be interested in using all principal
  components, but just enough to get a ``good'' understanding of the
  data 92.
\item
  A \textbf{\emph{scree plot}}, which plots \(\text{PVM}_m\) vs.~\(m\),
  can help identify a good number of principal components to use, is one
  visual method for identifying a good number of principal components.
  We look for an \textbf{\emph{elbow}} - a value of \(m\) such that
  \(\text{PVM}_m\) drops off thereafter.
\item
  In general, the question of how many principal components are
  ``enough'' is ill-defined, and depends on the application and the
  dataset. We maybe look at the first few principal components in order
  to find interesting patterns. If none are evident, then we conclude
  further components are unlikely to be of use. If some are evident, we
  continue looking at components until no more interesting patterns are
  found.
\item
  In an unpervised setting, these methods are all ad hoc, and reflect
  the fact that PCA is generally used in EDA 93.
\end{itemize}

    \hypertarget{other-uses-for-principal-components}{%
\subsubsection{Other Uses for Principal
Components}\label{other-uses-for-principal-components}}

    \begin{itemize}
\tightlist
\item
  Many statistical techniques (regression, classification, clustering)
  can be adapted to the \(n \times M\) PCA matrix with columns the first
  \(M << p\) principal component score vectors.
\item
  The PCA matrix can be seen as a ``de-noising'' 94 of the original
  data, since the signal (as opposed to the noise) is weighted towards
  the earlier principal components
\end{itemize}

    \hypertarget{clustering-methods}{%
\subsection{Clustering Methods}\label{clustering-methods}}

    \begin{itemize}
\tightlist
\item
  This is a broad set of techniques for finding \textbf{\emph{clusters}}
  (or subgroups) of the data set.
\item
  Observations should be ``similar'' within clusters and dissimilar
  across clusters. The definition of ``similar'' is context dependent.
\item
  Clustering is popular in many fields, so there exist a great number of
  methods.
\end{itemize}

    \hypertarget{k-means-clustering}{%
\subsubsection{\texorpdfstring{\(K\)-Means
Clustering}{K-Means Clustering}}\label{k-means-clustering}}

    \begin{itemize}
\tightlist
\item
  \textbf{\emph{\(K\)-means clustering}} seeks to partition the data
  into a pre-specified number \(K\) of distinct, non-overlapping
  clusters.
\item
  More precisely, we seek a partition \(\hat{C}_1, \dots \hat{C}_K\) of
  the set of indices \(\{1, \dots n\}\)
\end{itemize}

\[\hat{C}_1, \dots \hat{C}_K = \underset{C_1, \dots, C_k}{\text{argmin}}\left(\sum_{k = 1}^K W(C_k)\right)\]

where \(W(C_k)\) is some measure of the variation within cluster
\(C_k\). - A typical choice of \(W(C_k)\) is the average 95 squared
Euclidean distance between points in \(C_k\):

\[W(C)_k = \frac{1}{|C_k|}\sum_{i, i' \in C_k} ||x_i - x_i'||^2\] - A
brute force algorithm for finding the global minimum is \(O(K^n)\) but
there is a much faster algorithm which is guaranteed to find a local
minimum. It uses a random initialization so it should be performed
several times.

    \hypertarget{algorithm-k-means-clustering}{%
\subparagraph{\texorpdfstring{Algorithm: \(K\)-Means
Clustering}{Algorithm: K-Means Clustering}}\label{algorithm-k-means-clustering}}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize by randomly assigning a cluster number \(1,\dots K\) to
  each observation.
\item
  While the cluster assignments change:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    For each \(k = 1, \dots K\), compute the \textbf{\emph{centroid}} of
    the \(k\)-th cluster (the vector of feature means for the
    observations in the cluster).
  \item
    Assign to each observation the number of the cluster whose centroid
    is closest.
  \end{enumerate}
\end{enumerate}

    \hypertarget{advantages}{%
\subparagraph{Advantages}\label{advantages}}

    \hypertarget{disadvantages}{%
\subparagraph{Disadvantages}\label{disadvantages}}

    \hypertarget{hierarchical-clustering}{%
\subsubsection{Hierarchical Clustering}\label{hierarchical-clustering}}

    \begin{itemize}
\tightlist
\item
  \textbf{\emph{Hierarchical clustering}} is an alternative clustering
  method which doesn't require a specified number of clusters and
  results in an attractive tree-based representation of the data called
  a \textbf{\emph{dendrogram}}.
\item
  \textbf{\emph{Bottom-up}} or \textbf{\emph{agglomerative}}
  hierarchical clustering builds a dendrogram from the leaves up to the
  trunk.
\end{itemize}

    \hypertarget{interpreting-a-dendrogram}{%
\subparagraph{Interpreting a
Dendrogram}\label{interpreting-a-dendrogram}}

    \begin{itemize}
\tightlist
\item
  A dendrogram is a tree (visualized as upside down) with leaves
  corresponding to observations.
\item
  As we move up the tree, similar observations fuse into branches, and
  similar branches again fuse.
\item
  The earlier fusions occur, the more similar the corresponding groups
  of observations. The height at which two observations are joined by
  this fusing is a measure of this similarity.
\item
  At each height in the dendrogram, a horizontal cut splits the
  observations into \(k\) clusters (corresponding to each of the
  branches cut) where \(1 \leqslant k \leqslant n\).
\item
  The best choice of cut (hence number \(k\) of clusters) is often
  obtained by inspecting the diagram.
\end{itemize}

    \hypertarget{the-hierarchical-clustering-algorithm}{%
\subparagraph{The Hierarchical Clustering
Algorithm}\label{the-hierarchical-clustering-algorithm}}

    \begin{itemize}
\tightlist
\item
  This algorithm uses a notion of dissimilarity defined for clusters,
  called a \textbf{\emph{linkage}}.
\item
  Let \(A, B\) be clusters, and let \(d(a, b)\) be a dissimilarity
  measure 95 for observations \(a, b\). A linkage defines a
  dissimilarity measure \(d(A,B)\) between the clusters \(A, B\). The
  four most common types of linkage are

  \begin{itemize}
  \tightlist
  \item
    \textbf{\emph{complete}}:
    \[d_{comp}(A, B) = \underset{(a, b) \in A \times B}{\max} d(a, b)\]
  \item
    \textbf{\emph{single}}:
    \[d_{sing}(A, B) = \underset{(a, b) \in A \times B}{\min} d(a, b)\]
  \item
    \textbf{\emph{average}}
    \[d_{avg}(A, B) = \frac{1}{|A||B|} \underset{(a, b) \in A \times B}{\sum} d(a, b)\]
  \item
    \textbf{\emph{centroid}} \[d_{cent}(A, B) = d(x_a, x_b)\],\\
    where \(x_a\) (resp. \(x_b\)) is the centroid of \(A\) (resp.
    \(B\)).
  \end{itemize}
\item
  Average, complete, and single linkages are preferred by statisticians.
  Average and complete linkages are generally preferred as they result
  in more balanced dendrograms.
\item
  Centroid linkage is often used in genomics, but suffers from the
  possibility of an \textbf{\emph{inversion}}, in which two clusters are
  fused at a height \emph{below} the individual clusters, which makes
  interpretation difficult.
\end{itemize}

    Choice of Dissimilarity Measure

    \begin{itemize}
\tightlist
\item
  The squared Euclidean distance is often used as a dissimilarity
  measure.
\item
  An alternative is the \textbf{\emph{correlation-based distance}}
\item
  The choice of dissimilarity measure is very important and has a strong
  effect on the resulting dendrogram. The choice of measure should be
  determined by context.
\item
  One should consider scaling the data before choosing the dissimilarity
  measure.
\end{itemize}

    Algorithm: Hierarchical Clustering

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize with \(n\) clusters, one for each observation, and compute
  the dissimilarities \(d(x_i, x_j)\) for each pair.
\item
  For \(i = n, \dots, 2\):

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Compute all dissimilarites among the \(i\) clusters, and fuse the
    two clusters which are the least dissimilar. This dissimilarity is
    the height in the dendrogram where the fusion is placed.
  \item
    Compute the dissimilarities among the new \(i -1\) clusters.
  \end{enumerate}
\end{enumerate}

    Advantages

    Disadvantages

    \hypertarget{practical-issues-in-clustering}{%
\subsubsection{Practical Issues in
Clustering}\label{practical-issues-in-clustering}}

    \hypertarget{small-decisions-with-big-consequences}{%
\subparagraph{Small Decisions with Big
Consequences}\label{small-decisions-with-big-consequences}}

    \begin{itemize}
\tightlist
\item
  Should observations or features be standardized in some way?
\item
  For hierarchical clustering:

  \begin{itemize}
  \tightlist
  \item
    What dissimilarity measure should be used?
  \item
    What type of linkage should be used?
  \item
    Where should we cut the dendrogram to determine the number of
    clusters?
  \end{itemize}
\item
  For \(K\)-means clustering, what is the choice of \(K\)?
\end{itemize}

    \hypertarget{validating-the-clusters-obtained}{%
\subparagraph{Validating the Clusters
Obtained}\label{validating-the-clusters-obtained}}

    \begin{itemize}
\tightlist
\item
  It is important to decide whether the clusters obtained reflect true
  subgroups in the data or are a result of ``clustering the noise''.
\item
  There exist techniques for making this decision, such as obtaining
  \(p\)-values for each cluster.
\end{itemize}

    \hypertarget{other-considerations-in-clustering}{%
\subparagraph{Other Considerations in
Clustering}\label{other-considerations-in-clustering}}

    \begin{itemize}
\tightlist
\item
  Both \(K\)-means and hierarchical clustering assign all observations
  to some cluster. This can be problematic, for example in the presence
  of outliers that don't clearly belong to any cluster.
\item
  ``Mixture models'' are an attractive approach to accommodating
  outliers (they amount to a ``soft'' clustering approach).
\item
  Clustering methods are not robust to perturbations.
\end{itemize}

    \hypertarget{a-tempered-approach-to-interpreting-the-results-of-clustering}{%
\subparagraph{A Tempered Approach to Interpreting the Results of
Clustering}\label{a-tempered-approach-to-interpreting-the-results-of-clustering}}

    \begin{itemize}
\tightlist
\item
  Clustering can be a very useful and valid statistical tool if used
  properly.
\item
  To overcome the sensitivity to hyperparameters, is recommended to try
  hyperparameter optimization.
\item
  To overcome the sensitivity to perturbations, it is recommended to
  cluster on subsets of the data.
\item
  Finally, results of cluster analysis should be considered a part of
  EDA and not taken too seriously
\end{itemize}

    \begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{footnotes}{%
\subsection{Footnotes}\label{footnotes}}

    \hypertarget{foot85}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{84}
\tightlist
\item
  The linear algebra interpretation is also nice ↩
\end{enumerate}

\hypertarget{foot86}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{85}
\tightlist
\item
  This constraint is equivalent to
\end{enumerate}

\[\text{corr}(Z_j, Z_{j-1}) = 0\] ↩

\hypertarget{foot87}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{86}
\tightlist
\item
  See book figure 10.1 and corresponding discussion. ↩
\end{enumerate}

\hypertarget{foot88}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{87}
\tightlist
\item
  That is, the linear subspace in \(\mathbb{R}^p\) which minimizes the
  sum of the squared euclidean distances to the points \(x_i\). ↩
\end{enumerate}

\hypertarget{foot89}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{88}
\tightlist
\item
  When \(M = \min\{n - 1, p\}\), the approximation is exact. ↩
\end{enumerate}

\hypertarget{foot90}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{89}
\tightlist
\item
  More accurately, the sum on the right is an estimate of the sum on the
  left.
\end{enumerate}

\hypertarget{foot91}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{90}
\tightlist
\item
  In general there are \(\min\{n-1, p\}\) principal components and
  \[\sum_{m = 1}^{\min\{n-1, p\}} \text{PVE}_m = 1\]
\end{enumerate}

\hypertarget{foot92}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{91}
\tightlist
\item
  Indeed, if we take \(M < p\) principal components, then we are truly
  doing dimensional reduction.
\end{enumerate}

\hypertarget{foot93}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{92}
\tightlist
\item
  In a supervised setting, however, we can treat the number of
  components as a tuning parameter.
\end{enumerate}

\hypertarget{foot94}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{93}
\tightlist
\item
  There is a nice information-theoretic interpretation of this
  statement.
\end{enumerate}

\hypertarget{foot95}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{94}
\tightlist
\item
  That we are taking an average is probably the reason for the ``means''
  in ``\(K\)-means''.
\end{enumerate}

\hypertarget{foot96}{}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{95}
\tightlist
\item
  For example, the commonly used squared Euclidean distance. See Choice
  of Dissimilarity Measure 
\end{enumerate}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
