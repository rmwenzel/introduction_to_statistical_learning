{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#An-Overview-of-Classification\" data-toc-modified-id=\"An-Overview-of-Classification-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>An Overview of Classification</a></span></li><li><span><a href=\"#Why-Not-Linear-Regression?\" data-toc-modified-id=\"Why-Not-Linear-Regression?-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Why Not Linear Regression?</a></span></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Logistic-Model\" data-toc-modified-id=\"The-Logistic-Model-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>The Logistic Model</a></span></li><li><span><a href=\"#Estimating-the-Regression-Coefficients\" data-toc-modified-id=\"Estimating-the-Regression-Coefficients-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Estimating the Regression Coefficients</a></span></li><li><span><a href=\"#Making-Predictions\" data-toc-modified-id=\"Making-Predictions-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>Making Predictions</a></span></li><li><span><a href=\"#Multiple-Logistic-Regression\" data-toc-modified-id=\"Multiple-Logistic-Regression-4.3.4\"><span class=\"toc-item-num\">4.3.4&nbsp;&nbsp;</span>Multiple Logistic Regression</a></span></li><li><span><a href=\"#Logistic-Regression-for-more-than-two-response-classes\" data-toc-modified-id=\"Logistic-Regression-for-more-than-two-response-classes-4.3.5\"><span class=\"toc-item-num\">4.3.5&nbsp;&nbsp;</span>Logistic Regression for more than two response classes</a></span></li></ul></li><li><span><a href=\"#Linear-Discriminant-Analysis\" data-toc-modified-id=\"Linear-Discriminant-Analysis-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Linear Discriminant Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bayes-Theorem-for-Classification\" data-toc-modified-id=\"Bayes-Theorem-for-Classification-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Bayes Theorem for Classification</a></span></li><li><span><a href=\"#Linear-Discriminant-Analysis-for-p=1\" data-toc-modified-id=\"Linear-Discriminant-Analysis-for-p=1-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Linear Discriminant Analysis for p=1</a></span></li><li><span><a href=\"#Linear-Discriminant-Analysis-for-p->-1\" data-toc-modified-id=\"Linear-Discriminant-Analysis-for-p->-1-4.4.3\"><span class=\"toc-item-num\">4.4.3&nbsp;&nbsp;</span>Linear Discriminant Analysis for p &gt; 1</a></span></li><li><span><a href=\"#Quadratic-Discriminant-Analysis\" data-toc-modified-id=\"Quadratic-Discriminant-Analysis-4.4.4\"><span class=\"toc-item-num\">4.4.4&nbsp;&nbsp;</span>Quadratic Discriminant Analysis</a></span></li></ul></li><li><span><a href=\"#A-Comparison-of-Classification-Methods\" data-toc-modified-id=\"A-Comparison-of-Classification-Methods-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>A Comparison of Classification Methods</a></span></li><li><span><a href=\"#Footnotes\" data-toc-modified-id=\"Footnotes-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Footnotes</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Logistic Regression\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Overview of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In ***classification*** we consider paired data $(\\mathbf{X}, Y)$, where $Y$ is a ***qualitative variable***, that is, a finite random variable.\n",
    "\n",
    "- The values $Y$ takes are called ***classes***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Not Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a linear regression model implies an ordering on the values of the response and in general there is no natural ordering on the values of a qualitative variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider a quantitiative predictor $X$ binary response variable $Y \\in {0,1}$\n",
    "\n",
    "- We want to model the conditional probability of $Y=1$ given $X$\n",
    "\n",
    "$$ P(X) := P\\left(Y=1 | X\\right)$$\n",
    "\n",
    "- We model $P(X)$ with the ***logistic function***\n",
    "\n",
    "$$ P(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} $$\n",
    "\n",
    "- The logistic model can be considered a linear model for the ***log-odds*** or ***logit***\n",
    "\n",
    "$$\\log\\left(\\frac{P(X)}{1 - P(X)}\\right) = \\beta_0 + \\beta_1 X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Regression Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The likelihood function for the logistic regression parameter $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)$ is\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell(\\boldsymbol{\\beta}) &= \\prod_{i = 1}^n p(x_i)\\\\\n",
    "&= \\prod_{i: y_i = 1}p(x_i) \\prod_{i: y_i = 0} (1 - p(x_i))\n",
    "\\end{align*}\n",
    "\n",
    "- The maximum likelihood estimate (MLE) for the regression parameter is\n",
    "\n",
    "$$ \\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\beta}\\in \\mathbb{R}^2}{\\text{argmax}\\,} \\ell(\\boldsymbol{\\beta})$$\n",
    "\n",
    "- There isn't a closed form solution for $\\hat{\\boldsymbol{\\beta}}$ so [it must be found using numerical methods](https://en.wikipedia.org/wiki/Logistic_regression#Maximum_likelihood_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MLE $\\hat{\\boldsymbol{\\beta}}$ results in an estimate for the conditional probability $\\hat{P}(X)$ which can be used to predict the class $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiple logistic regression considers the case of multiple predictors $\\mathbf{X} = (X_1,\\dots, X_p)^\\top$.\n",
    "\n",
    "- If we write the predictors as $\\mathbf{X} = (1, X_1, \\dots, X_p)^\\top$, and the parameter $\\boldsymbol(\\beta) = (\\beta_0, \\dots, \\beta_p)^\\top$ then multiple logistic regression models\n",
    "\n",
    "$$p(X) = \\frac{\\exp(\\boldsymbol{\\beta}^\\top \\mathbf{X})}{1 + \\exp(\\boldsymbol{\\beta}^\\top \\mathbf{X})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for more than two response classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This isn't used often (a [softmax](https://en.wikipedia.org/wiki/Softmax_function) is often used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a method for modeling the conditional probability of a qualitative response $Y$ given quantitative predictors $\\mathbf{X}$ when $Y$ takes more than two values. It is useful because:\n",
    "\n",
    "- Parameter estimates for logistic regression are suprisingly unstable when the classes are well separated, but LDA doesn't have this problem\n",
    "\n",
    "- If $n$ is small and the $X_i$ are approximately normal in the classes (i.e. the conditional $X_i | Y = k$ is approximately normal) LDA is more stable\n",
    "\n",
    "- LDA can accomodate more than two clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider a quantitiative input $\\mathbf{X}$ and qualitative response $Y \\in {1, \\dots K}$. \n",
    "\n",
    "- Let $\\pi_k := \\mathbb{P}(Y = k)$ be the prior probability that $Y=k$, let $p_k(x) := \\mathbb{P}(Y = k\\ |\\ X = x)$ be the posterior probability that $Y = k$, and let $f_k(x):= \\mathbb{P}(X = x\\ |\\ Y = k)$. Then Bayes' theorem says:\n",
    "\n",
    "$$ p_k(x) = \\frac{\\pi_k f_k(x)}{\\sum_{l}\\pi_l f_l(x)} $$\n",
    "\n",
    "- We can form an estimate $\\hat{p}_k(x)$ for $p_k(x)$ with estimates of $\\pi_k$ and $f_k(x)$ for each k, and for $x$ predicts <sup><a href='#foot25' id='ref25'>25</a></sup>  \n",
    "\n",
    "$$\\hat{y} = \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\,} \\hat{p}_k(x) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis for p=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume that the conditional $ X| Y = k \\sim \\text{Normal}(\\mu_k, \\sigma_k^2)$ and that the variances are equal across classes $\\sigma_1^2 = \\cdots = \\sigma_K^2 = \\sigma^2$.\n",
    "\n",
    "- The Bayes classifier  predicts $Y = k$ where $p_k(x)$ is largest or equivalently\n",
    "\\begin{align*}\n",
    "\\hat{y}\n",
    "&= \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\ } \\delta_k(x)\\\\\n",
    "\\end{align*}\n",
    "    where\n",
    "$$ \\delta_k(x) := \\left(\\frac{\\mu_k}{\\sigma^2}\\right) - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)$$\n",
    "    is the ***discriminant function***<sup><a href='#foot26' id='ref26'>26</a></sup>.\n",
    "\n",
    "- The LDA classifier <sup><a href='#foot27' id='ref27'>27</a></sup> estimates the parameters \n",
    "\\begin{align*}\n",
    "\\hat{\\mu}_k &= \\frac{1}{n_k}\\sum_{i: y_i = k} x_i\\\\\n",
    "\\hat{\\sigma}_k &= \\frac{1}{n-K} \\sum_{k = 1}^K \\sum_{i: y_i = k} \\left(x_i - \\hat{\\mu}_k\\right)^2\n",
    "\\end{align*}\n",
    "Where $n_k$ is the number of observations in class $k$ <sup><a href='#foot28' id='ref28'>28</a></sup> and predicts \n",
    " \n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\ } \\hat{\\sigma}_k(x)) \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis for p > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume that the conditional $(\\mathbf{X}| Y = k) \\sim \\text{Normal}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$ and that the covariance matrices are equal across classes $\\boldsymbol{\\Sigma}_1 = \\cdots = \\boldsymbol{\\Sigma}_K = \\boldsymbol{\\Sigma}$.\n",
    "\n",
    "- The discriminant functions are\n",
    "\n",
    "$$\\sigma_k(x) = x^\\top\\boldsymbol{\\Sigma}^{-1}\\mu_k - \\frac{1}{2}\\mu_k {\\Sigma}^{-1} \\mu_k + \\log(\\pi_k)$$\n",
    "\n",
    "- LDA estimates $\\boldsymbol{\\mu}_k$ and $\\boldsymbol{\\Sigma}_k$ componentwise \n",
    "\\begin{align*}\n",
    "(\\hat{\\mu}_k)_j &= \\frac{1}{n_k}\\sum_{i: y_i = k} x_{ij}\\\\\n",
    "(\\hat{\\sigma}_k)_j &= \\frac{1}{n-K} \\sum_{k = 1}^K \\sum_{i: y_i = k} \\left(x_{ij} - (\\hat{\\mu}_k)_j\\right)^2\n",
    "\\end{align*}\n",
    "    for $1 \\leqslant j \\leqslant p$ as above and predicts\n",
    "\\begin{align*}\n",
    "\\hat{y} &= \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\ } \\hat{\\sigma}_k(x)) \\\\\n",
    "\\end{align*}\n",
    "    as above\n",
    "\n",
    "- Confusion matrices help analyze misclassifications for an LDA model <sup><a href='#foot29' id='ref29'>29</a></sup>\n",
    "\n",
    "- The Bayes decision boundary may not be agreeable in every context so sometimes a different decsision boundary (threshold) is used. \n",
    "\n",
    "- An ROC curve is useful for vizualising true vs false positives over different decision thresholds in the binary response case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assume that the conditional $(\\mathbf{X}| Y = k) \\sim \\text{Normal}(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$ but assume that the covariance matrices $\\boldsymbol{\\Sigma}_k$ are not equal across classes \n",
    "\n",
    "- The discriminant functions are now quadratic in $x$\n",
    "\n",
    "$$\\sigma_k(x) = x^\\top\\boldsymbol{\\Sigma}_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k {\\Sigma}_k^{-1} \\mu_k + \\log(\\pi_k)$$\n",
    "\n",
    "- QDA has  more degrees of freedom than LDA <sup><a href='#foot30' id='ref30'>30</a></sup> so generally has lower bias but higher variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Comparison of Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far our classification methods are KNN, logistic regression (LogReg), LDA and QDA\n",
    "\n",
    "- LDA and LogReg both produce linear decision boundaries. They often give similar performance results, although LDA tends to outperform when the conditionals $X | Y = k$ are normally distributed, and not when they aren't\n",
    "\n",
    "- As a non-parametric approach, KNN produces a non-linear decision boundary, so tends to outperform LDA and LogReg when the true decision boundary is highly non-linear. It doesn't help with selecting important predictors\n",
    "\n",
    "- With a quadratic decision boundary, QDA is a compromise between the non-linear KNN and the linear LDA/LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Footnotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "</p>\n",
    "\n",
    "<div id=\"foot25\"> 25. Recall that that the Bayes classifier predicts\n",
    "    \n",
    "$$\\hat{y} = \\underset{1 \\leqslant k \\leqslant K}{\\text{argmax}\\,} p_k(x) $$\n",
    "\n",
    "So we can think of LDA as an esimate of the Bayes Classifier\n",
    "<a href=\"#ref25\">↩</a>\n",
    "</div>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "<div id=\"foot26\"> 26. The Bayes decision boundary corresponds to the parameter values $\\boldsymbol{\\mu} = (\\mu_1, \\dots, \\mu_K)$, $\\boldsymbol{\\sigma} = (\\sigma_1, \\dots, \\sigma_K)$ such that $\\delta_k(x) = \\delta_j(x)$ for all $1 \\leqslant j,k \\leqslant K$. The Bayes classifier assigns a class $y$ to an input $x$ based on where $x$ falls with respect to this boundary.\n",
    "    \n",
    "For the case $K=2$, this is equivalent to assigning $x$ to class 1 if $2x(\\mu_1 - \\mu_2) > \\mu_1^2 - \\mu_2^2$.\n",
    "<a href=\"#ref26\">↩</a>\n",
    "</div>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "<div id=\"foot27\"> 27. The functions $\\hat{\\delta}_k(x)$ are called ***discriminant*** functions, and since they're linear in $x$, the method is called linear discriminant analysis.\n",
    "<a href=\"#ref27\">↩</a>\n",
    "</div>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "<div id=\"foot28\"> 28. $\\hat{\\mu}_k$ is the average of all observation inputs in the $k$-th class, and $\\hat{\\sigma}_k$ is a weighted average of the samples variances over the $K$ classes,  \n",
    "<a href=\"#ref28\">↩</a>\n",
    "</div>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "<div id=\"foot29\"> 29. False positives and false negatives in the binary case.\n",
    "<a href=\"#ref29\">↩</a>\n",
    "</div>\n",
    "\n",
    "<p>\n",
    "</p>\n",
    "\n",
    "<div id=\"foot30\"> 30. $K\\binom{p}{2}$ for QDA versus $\\binom{p}{2}$ for LDA.\n",
    "<a href=\"#ref30\">↩</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": "4",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "541px",
    "left": "998px",
    "top": "133px",
    "width": "256px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
